.. role:: raw-html-m2r(raw)
   :format: html


Urban System Theory
===================

Urban scaling law
-----------------

About British cities â€“ prior to the industrial revolution, Britain had one big city â€“ London and a plausible hypothesis but we canâ€™t test it because of lack of data, is that then british cities followed a rank size rule. But the cities of the industrial revolution were not formed from a central place theory (which is consistent with Zipf); they were built on the coalfield which is where all the northern cities are. The midlands was built also on coal and iron while Liverpool (as well as Bristol) where the ports. Basically our problems with validating Zipf, scaling size and so on in Britain (England) have something to do with historial development â€“ history matters â€“ or as we say in complexity theory, the evolution of the city system is path dependent

Fabulous question Zhengzi! This is another limitation of this type of analysis, it does not take into account the interaction between cities. But of course it matters, since this is linked to growth and spillovers, so the task is to find the impact, and this is still a research question!

There are really three types if scaling at least 1) inverse power laws that relate to size distributions like cities â€“ Zipf â€“  2) scaling power laws that relate one variable to another â€“ say income as a function of population â€“ allometry â€“ but there is a third â€“ that we havenâ€™t talked about today that is really important and central to spatial interaction â€“ gravity transport models â€“ the inverse square law that relates to what you are saying I think and we will look at this in urban simulation

Geography Information Science
=============================

Spatial Patterns and Spatial Autocorrelation
--------------------------------------------


* The importance of pattern
* Patterns of categorical point data â€“ Point Pattern Analysis

  * Quadrat Analysis
  * Ripley's K
  * DBSCAN

* Patterns of spatially referenced continuous observations

  * Spatial autocorrelation
  * Defining near and distant things
  * Measuring spatial autocorrelation

    * Moran's I
    * LISA

The importance of pattern
^^^^^^^^^^^^^^^^^^^^^^^^^

Discrete Objects: Point Pattern Analysis
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

*John Snow* : Broad street pump

Quadrat Analysis
~~~~~~~~~~~~~~~~

Ripley's K
~~~~~~~~~~

DBSCAN
~~~~~~

`Landscapetoolbox <https://wiki.landscapetoolbox.org/doku.php/spatial_analysis_methods:home>`_

Continuous Objects:
^^^^^^^^^^^^^^^^^^^

Spatial autocorrelation
~~~~~~~~~~~~~~~~~~~~~~~

`GeoDa <https://geodacenter.github.io/documentation.html>`_

Defining near and distant things
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Measuring spatial autocorrelation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Moran's I
"""""""""

LISA
""""

ggplot 2 seminar
~~~~~~~~~~~~~~~~

https://binchi1990.github.io/CASA_seminar2/challenges.html##challenge-1-reorder-the-items-in-legend

Quantitative Methods
====================

An Introduction to Quantitative Thinking
----------------------------------------


#. Understand the structure and focus of the module.
#. Consider the different roles of quantitative methods and their relevance in your work.
#. Develop a method for tackling quantitative problems.
#. Consider how to formulate a research question and structure a piece of quantitative writing.


.. image:: http://www.zzzhou.me/images/2020/11/10/image-20201107210923020.png
   :target: http://www.zzzhou.me/images/2020/11/10/image-20201107210923020.png
   :alt: 


Fermi Estimation Problems
^^^^^^^^^^^^^^^^^^^^^^^^^

A process for tackling questions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


#. State Assumptions.
#. Establish what you know (facts, formulae etc).
#. Establish what you don't know, but need to know.
#. Sort out 3 by estimating, calculating or researching.
#. Put all back together to get an answer.
#. Think about whether your answer is sensible. Verify if possible.

Good practice of a quantitative essay
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Introduction
~~~~~~~~~~~~


* 
  Gives the reader a feel for your investigation.

* 
  Begins to justify your investigation


.. image:: http://www.zzzhou.me/images/2020/11/10/clip_image001.png
   :target: http://www.zzzhou.me/images/2020/11/10/clip_image001.png
   :alt: 


Literature Review
~~~~~~~~~~~~~~~~~


* Sources for context vs.Â sources for methods
* Is all the literature relevant to your investigation?
* How has your reading informed your methodology?
* Demonstrates the importance of your investigation
* Takes a critical perspective 
* (Engaging narrative)
* don't cite wikipedia


.. image:: http://www.zzzhou.me/images/2020/11/10/clip_image002.png
   :target: http://www.zzzhou.me/images/2020/11/10/clip_image002.png
   :alt: 


Research question
~~~~~~~~~~~~~~~~~


* explicit
* precise

  * real world relevance
  * informed by literature review
  * possibly split into sub-questions

Presentation of data
~~~~~~~~~~~~~~~~~~~~


* explains and visualize your data for the reader

Methodology
~~~~~~~~~~~


* Precise
* Reproducible
* Explicitly informed by literature review
* Explicitly designed to answer research question


.. image:: http://www.zzzhou.me/images/2020/11/10/clip_image003.png
   :target: http://www.zzzhou.me/images/2020/11/10/clip_image003.png
   :alt: 


Results
~~~~~~~


* Clear
* Complete
* Minimal Text
* Presented in most informative way possible

Discussion
~~~~~~~~~~


* Highlights key points from results
* Interprets and synthesises
* Acknowledges limitations
* Relates back to research question

Conclusion
~~~~~~~~~~


* Explicitly answers research question (as far as possible)
* Proposes further research


.. image:: http://www.zzzhou.me/images/2020/11/10/clip_image004.png
   :target: http://www.zzzhou.me/images/2020/11/10/clip_image004.png
   :alt: 


Overall Style
~~~~~~~~~~~~~


* Impersonal
  Use "we" or the passive ("A simulation was run.")
  Never use "I"
* 
  Not narrative
  Your Goal -- Communicate your results
  Not your goal -- Describe your every thought and action in chronological order

* 
  OBJECTIVES


  #. Understand the structure and focus of the course.
  #. Consider the different roles of quantitative methods and their relevance in your work.
  #. Develop a method for tackling quantitative problems.
  #. Consider how to formulate a research question and structure a piece of quantitative writing.

A Good Research Question...
^^^^^^^^^^^^^^^^^^^^^^^^^^^


* ... cannot be answered with a single calculation ;
* ... is not vague ; (it could be quite long to include detail precision)
* ... asks about the real world, not just about data or methods.
* ... is interesting and valuable.
* ... is ambitious.

Lecture 1 -- Assignment
^^^^^^^^^^^^^^^^^^^^^^^

Read one of the example QM Projects* provided on Moodle. With reference to the mark scheme for written work, consider what mark you would award to the project in the streams 'Context and Review' and Communication'. Come to the next lecture prepared to talk about your chosen mark and why you have awarded it. (You may not understand the techniques used, but do not worry about this. Focus on the structure and the way that literature is used to support the work.)

Approaching & Communicating Data
--------------------------------

Objectives
^^^^^^^^^^


#. Understand basic data types 4ä¸­æ•°æ®ç±»å‹
#. Consider how to summarise and represent data. æ•°æ®æè¿°æ–¹å¼
#. Understand basic distributions of data. 4ç§åˆ†å¸ƒ
#. Recognise three types of relationship between data series ä¸‰ç§å…³ç³»

Assignment Discussion
^^^^^^^^^^^^^^^^^^^^^

Data types
^^^^^^^^^^

Source : `Level of measurement <https://en.wikipedia.org/wiki/Level_of_measurement>`_


* æœ‰åºå˜é‡å¯ç”¨å‡å€¼ï¼Ÿé—´éš”å˜é‡å¯ç”¨å‡å€¼ï¼Ÿ
* 2020å¹´æ˜¯1010å¹´çš„ä¸¤å€ å¹´åœ¨åšæ ‡å¿—å’Œåšåº¦é‡çš„åŒºåˆ«


.. image:: http://www.zzzhou.me/images/2020/11/10/image-20201108104339605.png
   :target: http://www.zzzhou.me/images/2020/11/10/image-20201108104339605.png
   :alt: 


Nominal data åä¹‰å˜é‡
~~~~~~~~~~~~~~~~~~~~~


* Differentiates items based only on namesï¼› No orderï¼›Also called categorical data
* Example:

  * names, gender, country names
  * åœŸåœ°åˆ©ç”¨åˆ†ç±»æ•°æ®

* Descriptive methods

  * Equality: 'apple' is not 'pear', 'apple' is 'apple'
  * Mode (the most common item)

Ordinal data æœ‰åºå˜é‡
~~~~~~~~~~~~~~~~~~~~~


* Allow for rank order, but not the relative degree of difference
* Example

  * measurement of opinions
  * é«˜ä¸­ä½ç¨‹åº¦

* Descriptive methods

  * Equality Mode
  * The median: middle-ranked item
  * Differences do not make sense, nor do means.

Interval data é—´éš”å˜é‡
~~~~~~~~~~~~~~~~~~~~~~


* Allow for the degree of difference between items, but not the ratio between them. The zero value is arbitrary.
* Example

  * temperature with the Celsius scale
  * æ—¥æœŸ

* Descriptive methods

  * Equality, mode, median
  * Addition, Arithmetic mean
  * Ratio? The coordinate of 8 is twice as far as that of 4

    * ä¸èƒ½è¯´20åº¦æ˜¯10åº¦çš„ä¸¤å€çƒ­

Ratio data æ¯”ç‡å˜é‡
~~~~~~~~~~~~~~~~~~~


* Allow for ratio between items; The zero value is unique and non-arbitrary.
* Example

  * mass, length, energy

* Descriptive methods

  * Equality, mode, median, arithmetic mean
  * Ratio

Descriptive statistics
^^^^^^^^^^^^^^^^^^^^^^

Mean median and mode
~~~~~~~~~~~~~~~~~~~~

https://statisticsbyjim.com/basics/measures-central-tendency-mean-median-mode/


* Outliers and skewed data have a smaller effect on the median

  * **When to use the median**\ :
    `Skewed <https://statisticsbyjim.com/glossary/skewed-data/>`_
    distribution, Continuous data, `Ordinal data <https://statisticsbyjim.com/glossary/ordinal-variables/>`_
  * **When to use the mode**\ :
    `Categorical data <https://statisticsbyjim.com/glossary/categorical-variables/>`_\ ,
    Ordinal data, Count data, Probability Distributions

* å¯¹äºæ­£æ€åˆ†å¸ƒï¼Œä¸‰è€…æ— å·®å¼‚ï¼›åæ€åˆ†å¸ƒï¼Œä¸­ä½æ•°è¾ƒå¥½ï¼›æœ‰åºæ•°æ®ï¼Œä¸­ä½æ•°æˆ–è€…ä¼—æ•°è¾ƒå¥½ï¼›ä¼—æ•°å¯ä»¥é€šè¿‡å¯†åº¦åˆ†å¸ƒè®¡ç®—

About DP and SF
~~~~~~~~~~~~~~~

â€¢ DP: decimal places 
â€¢ SF: significant figures

Variance and std: Quantifying Spread
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Sample and Population
"""""""""""""""""""""

https://online.stat.psu.edu/stat200/book/export/html/21

.. image:: https://online.stat.psu.edu/stat200/sites/stat200/files/inline-images/InferenceGraphicSU17.png
   :target: https://online.stat.psu.edu/stat200/sites/stat200/files/inline-images/InferenceGraphicSU17.png
   :alt: 


Bessel's Correction
"""""""""""""""""""


.. image:: http://www.zzzhou.me/images/2020/11/10/image-20201108083413348.png
   :target: http://www.zzzhou.me/images/2020/11/10/image-20201108083413348.png
   :alt: 



* æ ·æœ¬ï¼š n-1
* å…¨é›†ï¼š n

Visualizing data
^^^^^^^^^^^^^^^^

Outliers
~~~~~~~~


* Tukey Fences

$$
IQR = UQ-LQ 
LQ - 1.5\times IQR,UQ+1.5\times IQR
$$

Boxplot
~~~~~~~

.. code-block:: python

   ## This program creates a boxplot from data stored in a csv file and saves it as a png image.

   ## The data file must be one column of numbers - no column labels, etc.
   ## It must be saved as a csv file (e.g. use "Save As" in Excel and choose csv format).
   ## It must be saved in the same folder as this program.
   ## See the file sample_boxplot_data.csv for reference.

   ## In the next line, replace sample_boxplot_data.csv with the filename of your data:
   data_filename = 'sample_boxplot_data.csv'

   ## In the next line, replace boxplot with the filename you wish to save as:
   output_filename = 'boxplot.png'

   ## Use the next line to set figure height and width (experiment to check the scale):
   figure_width, figure_height = 4,10

   ## You can ignore these two lines:
   import matplotlib.pyplot as plt
   import numpy as np

   data = np.genfromtxt(data_filename)

   ## If there are errors importing the data, you can also copy the data in as a list.
   ## e.g. data = [1.95878982, 2.59203983, 1.22704688, ...]

   ## This line creates the figure. 
   plt.figure(figsize=(figure_width,figure_height))

   ## Uncomment the next three lines to set the axis limits (otherwise they will be set automatically):
   ##axis_min = 0.95
   ##axis_max = 4.05
   ##plt.ylim([axis_min,axis_max])

   ## The next lines create and save the plot:
   plt.xlim([0.75,1.25])
   plt.xticks([])
   plt.boxplot(data, manage_ticks=(False))
   plt.savefig(output_filename)

Distributions
^^^^^^^^^^^^^


* mean,median,mode å’Œ boxplot æ˜¯å¯¹äºæ•°æ®çš„é€šç”¨æ–¹æ³•ï¼Œç®€å•æ–¹æ³•
* å¦ä¸€ç§æè¿°æ•°æ®çš„æ–¹æ³•-- probability distribution Probability density  function
* æ¦‚ç‡åˆ†å¸ƒä¸ç›´æ–¹å›¾çš„åŒºåˆ«ï¼Ÿ

  * å¦‚æœè¦æè¿°æ ·æœ¬ï¼Œåˆ™ä½¿ç”¨ç»éªŒåˆ†å¸ƒï¼ˆå³ç›´æ–¹å›¾ï¼‰ï¼Œå¦‚æœè¦æè¿°å‡è®¾çš„åŸºç¡€åˆ†å¸ƒï¼Œåˆ™ä½¿ç”¨pdfã€‚
  * æ¦‚ç‡è´¨é‡å‡½æ•°ï¼ˆpmfï¼‰é’ˆå¯¹çš„æ˜¯ç¦»æ•£å˜é‡ï¼Œä¾‹å¦‚æ³Šæ¾åˆ†å¸ƒã€‚
  * å¤§å­¦è¯¾ç¨‹ https://ocw.tudelft.nl/courses/observation-theory-estimating-unknown/

* å¦‚ä½•åˆ¤æ–­æ˜¯å¦åˆ†å¸ƒ

  * å›¾å½¢éªŒè¯

    * ç›´æ–¹å›¾
    * pdf
    * ç»éªŒç´¯ç§¯æ¦‚ç‡å›¾ : ä¹Ÿèƒ½æ‹Ÿåˆå…¶ä»–åˆ†å¸ƒ
    * å˜æ¢åæ ‡è½´çš„ç»éªŒç´¯ç§¯æ¦‚ç‡å›¾
    * P-På›¾å’ŒQ-Qå›¾

  * ç»Ÿè®¡æ£€éªŒ

    * Kolmogorov-Smirnovæ£€éªŒ
    * Anderson-Darlingæ£€éªŒ
    * Shapiro-Wilkæ£€éªŒ

* https://docs.scipy.org/doc/scipy/reference/stats.html

Normal Distribution
~~~~~~~~~~~~~~~~~~~

Log-normal
~~~~~~~~~~


* ä¸å¯å°äº0 ä¸€äº›å€¼éå¸¸å¤§


.. image:: https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/PDF-log_normal_distributions.svg/600px-PDF-log_normal_distributions.svg.png
   :target: https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/PDF-log_normal_distributions.svg/600px-PDF-log_normal_distributions.svg.png
   :alt: 


Poisson
~~~~~~~


* Poisson Distribution gives the probability of k events.
* how many times things happen?
* 
  .. image:: https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/Poisson_pmf.svg/650px-Poisson_pmf.svg.png
     :target: https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/Poisson_pmf.svg/650px-Poisson_pmf.svg.png
     :alt: 

Exponential distribution
~~~~~~~~~~~~~~~~~~~~~~~~


* The Exponential distribution is linked to the Poisson distribution.
* If the Poisson measures the probability of x events within a time period, then the Exponential measures how long we are likely to wait between events.
* So imagine an emergency room in a hospital. The exponential distribution will tell us how likely we are to wait x amount of time from one patient arriving to the next.

:raw-html-m2r:`<u>Guessing Distribution</u>`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


#. Shoe size normal
#. The thickness of tree bark lognormal
#. The number of times a website is accessed per minute. poisson
#. The time between phone calls at a call centre. exponential
#. The number of popcorn kernels popping in a 5 second window
   **normal** ï¼Ÿï¼Ÿ poisson

   * è¿™æ˜¯ä¸ªæ—¶é—´åºåˆ—è€Œä¸æ˜¯å•ä½æ—¶é—´å‘ç”Ÿçš„æ¬¡æ•°

https://www.youtube.com/watch?v=YmOsDTczOFs&ab_channel=MindYourDecisions

Boxcox transformæ–¹æ³•
~~~~~~~~~~~~~~~~~~~~


* åªæ˜¯ä¸€ç§å°†æ•°æ®è½¬æ¢ä¸ºæ­£æ€åˆ†å¸ƒçš„æ–¹æ³•
* æ”¹å˜äº†æ•°æ®çš„æ„ä¹‰

Relationships
^^^^^^^^^^^^^


* ä¸‰ç§å…³ç³»
* å¦‚ä½•åˆ¤æ–­å…³ç³»

  * using logarithms to straighten curves
  * If your data makes a straight line on a log-linear plot, it may be an exponential relationship
  * If your data makes a straight line on a log-log plot, it may be an power law relationship
  * å¦‚æœæ•°æ®åœ¨yå¯¹æ•°å›¾ä¸Šæ˜¯ä¸€æ¡ç›´çº¿ï¼Œé‚£ä¹ˆå®ƒå¯èƒ½æ˜¯æŒ‡æ•°å…³ç³»
  * åŒå¯¹æ•°å›¾ä¸Š-- å¹‚å¾‹

* zipf's law

Linear
~~~~~~

Bus stops in a line road and in a spatial circle

Kleiber's Law and power law
~~~~~~~~~~~~~~~~~~~~~~~~~~~

$$
Metabolic rate = M \times x^{n} \
x: body mass\
n: power
$$
Power law


* for Cities
* Linear relationships are power laws!
* å¯¹äºå¹‚å¾‹ï¼Œè¿™ç§å…³ç³»å¯èƒ½åªé€‚ç”¨äºè¶…è¿‡ä¸€å®šå¤§å°çš„å€¼ã€‚è€ƒè™‘å¿½ç•¥å°çš„ä»·å€¼
* å¯¹äºæ—¶é—´æ•°æ®(ä¾‹å¦‚å¹´ä»½)ï¼Œå¯èƒ½æ²¡æœ‰è‡ªç„¶çš„"é›¶"ã€‚è€ƒè™‘ä»ä¸€ä¸ªåˆç†çš„åŸºå‡†æ—¶é—´åº¦é‡ã€‚(ä¾‹å¦‚ï¼Œå¦‚æœæ•°æ®å§‹äº1900å¹´ï¼Œä½¿ç”¨"è‡ª1900å¹´ä»¥æ¥çš„å¹´ä»½")
* kleiber's lawéƒ½æ˜¯power law çš„ç‰¹æ®Š
* zipf's law

Exponentials
~~~~~~~~~~~~

..

   " The greatest shortcoming of the human race is our inability to understand the exponential function

   â€‹                                                                                                                                                         -- Albert Bartlett


----

:raw-html-m2r:`<u>Suggested Reading/Viewing</u>`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


* lectures
* `Padlet for disscussion qm projects <https://padlet.com/hannahfry/bbua7lcjtkyq6mh4>`_
* `Logarithms from Khan Academy <https://www.khanacademy.org/math/algebra2-2018/exponential-and-logarithmic-functions/introduction-to-logarithms/v/logarithms>`_
* Assignments

----

Basic Mathematical Catch-Up


* Introduction to logarithms: http://www.purplemath.com/modules/logs.htm
* A straightforward discussion of variance and standard deviation: averages, their advantages and disadvantages: 
* Box plots: http://www.physics.csbsju.edu/stats/box2.html

Variance - Deeper Thinking


* For those who know a bit of stats and have read that the sample variance should use n-1 as a divisor:
  http://www.crataegus.me.uk/thoughts/bessel_correction.html

Excel Tutorial


* For those not comfortable with Excel, here is a short entry-level tutorial on using Excel formulae:
  http://www.excel-easy.com/introduction/formulas-functions.html
* That site has loads more Excel advice, so have a look around there if you like.

Power Laws


* A TED talk on power laws for cities, given by one of the authors of the paper that I referred to in the lecture. Entertaining, relevant and informative and only 17 minutes long: https://www.ted.com/talks/geoffrey_west_the_surprising_math_of_cities_and_corporations\##t-173864
* Here is a nice tutorial on power laws and log-log plots from the University of Plymouth. It goes into a little more detail than we did, so it is good for people who had trouble following (skip the bits that don't relate to what was covered in the lecture) or who are confident and want to extend/test their knowledge.http://www.reading.ac.uk/AcaDepts/sp/PPLATO/imp/interactive%20mathematics/loglog2.pdf
* Here is an academic paper on power laws in cities. It is well worth a read for background knowledge, but do not worry if you do not understand all of it (skip the mathematical section: "Urban Growth Equation" on p7304, unless you are a mathematician or a masochist). http://www.pnas.org/content/104/17/7301.full.pdf
* There is also an accessible summary of the paper here:http://scienceblogs.com/purepedantry/2007/10/18/power-laws-and-cities/
* But note that there is a mistake in that summary. When they talk about "exponential" increases, they mean "superlinear". The academic paper does not discuss any exponential relationships.

----

Distributions

**Light Accessible Reading/Viewing**

Zipf's Law:


* https://plus.maths.org/content/mystery-zipf
* http://io9.com/the-mysterious-law-that-governs-the-size-of-your-city-1479244159
  Video: https://youtu.be/fCn8zs912OE?t=4

**Tutorials**
Poisson Distribution (Good accessible overview):
https://www.umass.edu/wsp/resources/poisson/
Stat Trek on Distributions:
http://stattrek.com/probability-distributions/probability-distribution.aspx?Tutorial=Stat
Normal Distribution (In significant depth):
http://onlinestatbook.com/2/normal_distribution/intro.html

**More Technical Background**
Lognormal across the
sciences:http://stat.ethz.ch/~stahel/lognormal/bioscience.pdf
Working with Power Laws:


* `http://www-personal.umich.edu/~mejn/courses/2006/cmplxsys899/powerlaws.pd <http://www-personal.umich.edu/~mejn/courses/2006/cmplxsys899/powerlaws.pdf>`_
* https://arxiv.org/pdf/0706.1062.pdf

**Further Academic Papers**
There is More than a Power Law in Zipf: http://www.nature.com/articles/srep00812

----

Albert Bartlett's lecture:https://youtu.be/O133ppiVnWY
BBC article on population projections: http://news.bbc.co.uk/1/hi/8000402.stm
Telegraph article on oil: http://www.telegraph.co.uk/news/earth/energy/oil/9867659/Why-the-world-isnt-running-out-of-oil.html

Assignment
^^^^^^^^^^


#. Watch Albert Bartlett's lecture on the exponential function: http://www.youtube.com/watch?v=F-QA2rkpBSY (You only need to watch the first 20 minutes or so.)
   Read the following articles:http://news.bbc.co.uk/1/hi/magazine/8000402.stm
   http://www.telegraph.co.uk/news/earth/energy/oil/9867659/Why-the-world-isnt-running-out-of-oil.html
   Consider what we should conclude about quantitative predictions?
#. Have a play with the excel data investigation. It will guide you through how to plot and explore data in Excel.
#. Look at the relationships data sets on moodle. What do they reveal about the underlying relationships?

Measuring Relationships
-----------------------

Objectives
^^^^^^^^^^


#. Understand the concept of covariance, correlation and what it says about data relationships. 
#. Be able to measure a data relationship using simple linear regression. 
#. Learn how regression can be extended for data with more than two dimensions. 
#. Understand how to identify whether additional data provides additional information.

Lectures
~~~~~~~~

How many ways do we have to measure correlation? What are the main differences between these measurements? What is partial correlation and how can we understand it intuitively?
We can't use correlation to predict, as correlation does not imply causation, but if variables are dependent then they are either causal to each other (in either direction) or driven by a common driver.
Do we need to perform a heteroskedasticity test before regression, and how can we do this?


* data type
* distribution
* relationship
* regression

Motivations for data transformation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


#. To formulate a different question
#. To meet the assumptions of a statistical inference procedure
#. To make data easier to visualise

Outliers and Dealing with it
~~~~~~~~~~~~~~~~~~~~~~~~~~~~


* å…ˆç”¨æ–¹æ³•è¯†åˆ«outliers ï¼Œæ ¹æ®ç»éªŒåˆ¤æ–­æ˜¯å¦ä¿ç•™
* types of outliers

  #. Error
  #. Points not following the pattern

     * If you are studying the overall pattern, they need to be removed -- e.g. City of London (in London) -- e.g. Vantican City (1000 pop, gender ratio 7:1)
     * "Statistical oddities"

  #. Points that are essential to the overall pattern

     * Don't remove them e.g. New York in Uï¼³ pop data

* There are many approaches to identify outliers (e.g. Tukey fences), but determining an outlier is a subjective exercise.
* Dealing with outliers (not sure type 2 or 3)

  * Retention, **but the application should use methods that are robust to outlier points**
  * Exclusion. If some outliers are excluded, the reasons should be clearly stated on the report.

----

Part 1. Covariance, Correlation, and Association
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Variability
~~~~~~~~~~~

a single variable
"""""""""""""""""

Co-variability of two variables
"""""""""""""""""""""""""""""""

ANOVA (not covered here)
""""""""""""""""""""""""

â€¢ Whether two or more population means are equal â€¢ When there are only two populations, it is called two sample t-test (covered in Lecture 5) 
â€¢ E.g. is the mortality rate of Covid-19 the same across different cities?

Covariance
""""""""""

"Do the two variables change in the same direction?" "To what extent do they change together?"


.. image:: http://www.zzzhou.me/images/2020/11/10/image-20201020174858181.png
   :target: http://www.zzzhou.me/images/2020/11/10/image-20201020174858181.png
   :alt: 



* Meaning
* Meaning of sign
* Range? -inf~inf
* Normalised?  no 
* Symmetric

Larger Variance  


.. image:: http://www.zzzhou.me/images/2020/11/12/image-20201112225329571.png
   :target: http://www.zzzhou.me/images/2020/11/12/image-20201112225329571.png
   :alt: image-20201112225329571


Correlation
~~~~~~~~~~~

Person Correlation
""""""""""""""""""

$$
cor(X,Y) = r(X,Y) = \frac{cov(X,Y)}{std(X)\times std(Y)}
$$

**limitations**


* The Pearson correlation coefficient measures only **linear association**\ : how nearly the data fall on a straight line.
* It is not a good summary of association if the scatterplot has a nonlinear (curved) pattern.
* When you present the correlation, remember to present a scatterplot.
* The Pearson correlation coefficient is appropriate only for interval or ratio variables, not nominal or ordinal variables -- even if their values are numerical.
* Note the difference between the data type of the variable and the value.


.. image:: http://www.zzzhou.me/images/2020/11/10/image-20201020174836667.png
   :target: http://www.zzzhou.me/images/2020/11/10/image-20201020174836667.png
   :alt: image-20201020174836667


Spearman's Rank Correlation Coefficient
"""""""""""""""""""""""""""""""""""""""

$$
r\ *{s}=\rho*\ {\mathrm{rg}\ *{X}, \mathrm{rg}*\ {Y}}=\frac{\operatorname{cov}\left(\mathrm{rg}\ *{X}, \mathrm{rg}*\ {Y}\right)}{\sigma\ *{\mathrm{rg}*\ {X}} \sigma\ *{\mathrm{rg}*\ {Y}}}
$$


* It is applicable for interval/ratio data

  * Transform the value into ranks;
  * Calculate the Rank correlation.

* But it is different from Pearson's correlation, and they are
  incomparable.
* 
  Less sensitive to outliers than Pearson correlation


  .. image:: http://www.zzzhou.me/images/2020/11/10/image-20201020175750498.png
     :target: http://www.zzzhou.me/images/2020/11/10/image-20201020175750498.png
     :alt: 


Limitations
"""""""""""


* Pearson correlation is not applicable for nominal/ordinal data.
* It measures only linear association, and is not a good summary for non-linear data.
* It is a measure of association, not causation. It is not robust to outliers, less robust than Spearman's rank correlation.

correlation is a global metric, it doesn't give the local relation.

----

Part 2. Linear Regression
^^^^^^^^^^^^^^^^^^^^^^^^^


#. Where to place the line?
#. How well does the line represent the data?


.. image:: http://www.zzzhou.me/images/2020/11/10/image-20201020181527698.png
   :target: http://www.zzzhou.me/images/2020/11/10/image-20201020181527698.png
   :alt: 


Necessary Conditions
~~~~~~~~~~~~~~~~~~~~


* Linear relationship exists
* Independent errors : one error is not affected by the value of another error 
* Normally distributed errors : for each value of x, the errors have a normal distribution about the regression line.
* Equal error variance for all x values :  the error value does not change because of the x values

NOT a Necessary Condition! : Normally Distributed Data

verifying the conditions

residual independent 

normally distributed 

equal variance

Linear regression exists
~~~~~~~~~~~~~~~~~~~~~~~~

The relationship between y and x is linear; that is, there is an equation, y=mx+c+Îµ that constitutes the population model.

Independent errors
~~~~~~~~~~~~~~~~~~


* The residuals are independent; the value of one error is not affected by the value of another error.
* In probability, two events A and B are independent if knowing that B happens does not alter the probability that A happens. E.g. , flipping two coins.
* A and B are independent if Pr[B]=0 or Pr[A|B] = Pr[A]
* But... how can we know that?

Normally distributed errors
~~~~~~~~~~~~~~~~~~~~~~~~~~~


* For each value of x, the errors have a normal distribution about the regression line. This normal distribution is centred on the regression line.
* It may be written as Îµ~ğ‘(0, Ïƒ2)

Equal error variance for all x values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


* The errors about the regression line do not vary with x; that is, V[Îµ|x] = Ïƒ2 residual plot

Part 3. Multiple Regression
^^^^^^^^^^^^^^^^^^^^^^^^^^^


#. Can we combine greenery and natural light to create a model for predicting wellbeing?
#. How does the combined model compare with individual models?
#. Can we put categorical variables into the model?

Question 1
""""""""""

With R2 values of 30-40%, both models leave a lot of variation unaccounted for. Could we do better, by using BOTH data series to explain the variation in the reported well-being?

Readings
^^^^^^^^

Tools and Techniques
~~~~~~~~~~~~~~~~~~~~

**Covariance and Correlation:**\ http://ci.columbia.edu/ci/premba_test/c0331/s7/s7_5.html
**Spearman's Rank Correlation Coefficient:**\ https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php

*As an aside, while there are lots of Spearman's Rank information resources online, it is difficult to find one that does not make misleading claims about (surprise, surprise) normality assumptions. Many resources claim that you would use Spearman rather than Pearson correlation if you think your data series are not normally distributed.
This is false. Pearson correlation does not require normally distributed data (though some associated statistical tests do). The following two resources, for example, would also be good if it were not for such claims:*

http://www.statstutor.ac.uk/resources/uploaded/spearmans.pdf
http://www.biostathandbook.com/spearman.html

**Linear Regression - Tutorials**
Here is an excellent course on regression. I would recommend that you use it as your main reference work for this subject. Lessons 1, 3 and 4 are most relevant to simple linear regression:
https://onlinecourses.science.psu.edu/stat501/lesson/1
`https://onlinecourses.science.psu.edu/stat501/lesson/3 <https://onlinecourses.science.psu.edu/stat501/lesson/1>`_
`https://onlinecourses.science.psu.edu/stat501/lesson/4 <https://onlinecourses.science.psu.edu/stat501/lesson/1>`_

The course also goes through multiple linear regression (Lessons 5-7) and discusses the use of categorical variables in detail (Lesson 8):
`https://onlinecourses.science.psu.edu/stat501/lesson/5 <https://onlinecourses.science.psu.edu/stat501/lesson/1>`_
`https://onlinecourses.science.psu.edu/stat501/lesson/6 <https://onlinecourses.science.psu.edu/stat501/lesson/1>`_
`https://onlinecourses.science.psu.edu/stat501/lesson/ <https://onlinecourses.science.psu.edu/stat501/lesson/1>`_\ 7

Here is a lesson on performing simple and multiple linear regression in Python. It is a lot more advanced than the Python we have covered, but may be of interest for those who are fairly confident with the language:
http://www.dataschool.io/linear-regression-in-python/

Stat Trek (http://stattrek.com/) is another good site for learning about concepts in statistics. Here is the start of their material on regression: http://stattrek.com/regression/linear-regression.aspx?Tutorial=Stat

[However, note that Stat Trek say "For any given value of X... the Y values [must be] roughly normally distributed", which is true (this is effectively equivalent to saying that the residuals must be normally
distributed), but they then imply that looking at a histogram of the data would help to confirm this, which is of course false. The histogram might show that the DATA looks normally distributed, but as we now know,  this is irrelevant! It's a residuals vs fits plot you need.]

Research Skills
~~~~~~~~~~~~~~~

UCL Library's video series on using Web of Science to search for academic publications
https://youtu.be/AgTk3kA

Just for fun
~~~~~~~~~~~~

Spurious Correlations: http://www.tylervigen.com/spurious-correlations
The Great Horse Manure Crisis:http://www.historic-uk.com/HistoryUK/HistoryofBritain/Great-Horse-Manure-Crisis-of-1894/

Advanced Regression
-------------------


* å›å½’éœ€ä¸éœ€è¦å˜é‡æ­£æ€åŒ–ï¼Ÿä»€ä¹ˆæ—¶å€™éœ€è¦æ­£æ€åŒ–ï¼Œæ ‡å‡†åŒ–

Assignment 1
------------

Hypothesis Testing
------------------

**1.**  State your hypotheses.

**2.**  State your significance level: ğ›‚

**3.**  What is the evidence (E)? (The Test Statistic)

**4.**  Calculate the probability of seeing evidence at least  as extreme as E, *if* H0 *is true*. (The â€œp-valueâ€)

**5.** If the p-value is smaller than the significance, reject  H0 and accept H1. Otherwise there is not enough  evidence to reject H0.


.. image:: http://www.zzzhou.me/images/2020/11/17/image-20201117181037540.png
   :target: http://www.zzzhou.me/images/2020/11/17/image-20201117181037540.png
   :alt: image-20201117181037540


Chi squared å¼€ threshold

what chance of getting these observed values?

Quiz


#. 
#. two 
#. a firm discrimination between 


* 

Cluster Analysis
----------------

Part 1: Hypothesis testing recap
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Yes, the first question is whether your INDEPENDENT variable is categorical or quantitative. The second is whether your DEPENDENT variable categorical or quantitative.

0.01 0.05 p value 

Part 2: Cluster analysis â€“ why should I care?
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Sorry, I am always get confused about the difference among Boxcox transformation, log transformation, z-scores and  standardisation, it seems that all of these are transforming things on the same scale, how to choose the proper method?

Sorry, I am always confused about the difference between Boxcox transformation, log transformation, z-scores, and standardization. It seems that all of these are transforming things on the same scale. How to choose the proper method?

Will the converted data lose some features

Part 3: Before you start
^^^^^^^^^^^^^^^^^^^^^^^^

å¦‚ä½•æ¯”è¾ƒè°æ›´å¼‚å¸¸ï¼Ÿ

æ ‡å‡†åŒ–ä¸ä¼šæŸå¤±ç‰¹å¾

Part 5: Hierarchical clustering
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Part 6: How good are your clusters?
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Part 7: Some tips and tricks for your written work
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Optimising Limited Resources
----------------------------

Modelling the World
-------------------

Statistical Traps & Advanced Topics
-----------------------------------

wk4
---


* VIF for multicollinearity

Introduction to Programming
===========================

Reading week
------------

In addition to looking for relevant content in [@dignazio:2020] (\ `URL <https://bookbook.pubpub.org/data-feminism>`_\ ), you will also want to check consider:

Dignazio:2020
^^^^^^^^^^^^^

Elwood:2017
^^^^^^^^^^^

Elwood:2018
^^^^^^^^^^^

Crawford : 2015 `DOI <https://doi.org/10.1007/s10708-014-9597-z>`_
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


* ontological, epistemological, and ethical challenges that arise when social media datasets are used to understand crisis events
* 


* [@Elwood:2017] `DOI <https://doi.org/10.1080/13658816.2017.1334892>`_
* [@Elwood:2018] `DOI <https://doi.org/10.1080/0966369X.2018.1465396>`_
* [@Bemt:2018] `DOI <https://doi.org/10.1080/03098265.2018.1436534>`_
* [@Amoore:2019] `DOI <https://doi.org/10.1177%2F0263276419851846>`_

Week 6ï¼šSpatial Data
--------------------

Lectures
^^^^^^^^

Mapping
~~~~~~~


* Purely Computational vs. Mostly Computational

  * Wider variety of output formats (e.g. Atlases, 3D/web).
  * Better support for 'finishing touches' (e.g. scalebars, north arrows, rule-based labels, etc.).
  * Better-quality output for less effort (e.g. Model Builder + QGIS styles).

* 
  Every Building in America


  * `Building footprints <https://github.com/Microsoft/USBuildingFootprints/>`_ collected by Microsoft, but presentation by New York Times `highlights society-nature interactions <https://www.nytimes.com/interactive/2018/10/12/us/map-of-every-building-in-the-united-states.html>`_.
  * 
    .. image:: http://www.zzzhou.me/images/2020/11/17/NYT-MS-History.png
       :target: http://www.zzzhou.me/images/2020/11/17/NYT-MS-History.png
       :alt: NYT-MS-History

* 
  A Deceptively Simple Problem

..

      We want to show data on a map in a way that is **both accurate and informative**.



* Classification

  * The greater the accuracy of a **choropleth** or other class-based map, the less itâ€™s possible generalise from it.
  * There is no 'right' way to group data into an arbitrary number of discrete classes (a.k.a. to generalise).

* Different ways of representation will be totally different

  * Judgement should based on distribution
  * Six Views of Employment

..

   ..

      Different colour and break schemes not only give us different **views** of the data, they give us different **understandings** of the data! Each potentially changes how the data looks and, consequently, how we perceive the distribution.



* Maps have a 'Rhetoric' `Do maps lie? <https://www.youtube.com/watch?v=G0_MBrJnRq0>`_ it always helps to look at a map with a critical  eye.

Exploratory Data Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~


* Epicyclic Feedback

.. list-table::
   :header-rows: 1

   * - 
     - Set Expectations
     - Collect Information
     - Revise Expectations
   * - Question
     - Question is of interest to audience
     - Literature search/experts
     - Sharpen question
   * - EDA
     - Data are appropriate for question
     - Make exploratory plots
     - Refine question or collect more data
   * - Modelling
     - Primary model answers question
     - Fit secondary models / analysis
     - Revise model to include more predictors
   * - Interpretation
     - Interpretation provides specific and meaningful answer
     - Interpret analyses with focus on effect and uncertainty
     - Revise EDA and/or models to provide more specific answers
   * - Communication
     - Process & results are complete and meaningful
     - **Seek feedback**
     - Revises anlyses or approach to presentation



* 
  Approaching EDA


  * From: `EDAâ€”Don't ask how, ask what <https://medium.com/towards-artificial-intelligence/exploratory-data-analysis-eda-dont-ask-how-ask-what-2e29703fb24a>`_\ :
  * Descriptive Statistics: get a high-level understanding of your dataset
  * Missing values: come to terms with how bad your dataset is
  * Distributions and Outliers: and why countries that insist on using different units make our jobs so much harder
  * Correlations: and why sometimes even the most obvious patterns still require some investigating

    * Preview data
    * Check total number of entries and column types
    * Check any null values
    * Check duplicate entries
    * Plot distribution of numeric data (univariate and pairwise joint distribution)
    * Plot count distribution of categorical data
    * Analyse time series of numeric data by daily, monthly and yearly frequencies

* 
  Signal & Noise  


  * Starting at a chart 

    * The problem of relying on statistics alone was amply illustrated by Anscombe's Quartet (1973)...
    * We *are not* very good at looking at spreadsheets. 
    * We *are* very good at spotting patterns visually.
    * ^ Or, as Albert Einstein reportedly said: "If I can't picture it, I can't understand it."

  * What Makes a Good Plot?

    * Serves a **purpose** â€” it is clear how it advances the argument in a way that could not be done in the text *alone*.
    * Contains only what is **relevant** â€” zeroes in on what the reader *needs* and is not needlessly cluttered.
    * Uses precision that is **meaningful** â€” 
    * Far too many charts or tables could be easily written up in a single sentence.
    * Far too many charts or tables contain redundancy, clutter, and 'flair'.\
    * Don't report average height of your class to sub-millimeter level accuracy, or lat/long to sub-atomic scale.

  * The Purpose of a Chart

    * Think of a chart or table as part of your â€˜argumentâ€™ â€“ if you canâ€™t tell me how a figure advances your argument (or if your explanation is more concise than the figure) then you probably donâ€™t need it.
    * Identify & prioritise the relationships in the data.
    * Choose a chart type/chart symbology that gives emphasis to the most important relationships.

  * Beyond the Chart
  * Why a table is sometimes better than a chart:

    * You need to present data values with greater detail
    * You need to enable readers to draw comparisons between data values
    * You need to present the same data in multiple ways (\ *e.g.* raw number *and* percentage)
    * You want to show many dimensions for a small number of observations
    * è¡¨æ ¼çš„æ ¼å¼  æ•°å­—å¯¹é½æ–¹å¼ ä¿¡æ¯ä¸é‡å¤ ä¸é—æ¼

Links


* `Pandas Reference <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html>`_
* `A Guide to EDA in Python <https://levelup.gitconnected.com/cozy-up-with-your-data-6aedfb651172>`_ (Looks very promising)
* `EDA with Pandas on Kaggle <https://www.kaggle.com/kashnitsky/topic-1-exploratory-data-analysis-with-pandas>`_
* `EDA Visualisation using Pandas <https://towardsdatascience.com/exploratory-data-analysis-eda-visualization-using-pandas-ca5a04271607>`_
* `Python EDA Analysis Tutorial <https://www.datacamp.com/community/tutorials/exploratory-data-analysis-python>`_
* `Better EDA with Pandas Profiling <https://towardsdatascience.com/a-better-eda-with-pandas-profiling-e842a00e1136>`_ **[Requires module installation]**
* `EDA: DataPrep.eda vs Pandas-Profiling <https://towardsdatascience.com/exploratory-data-analysis-dataprep-eda-vs-pandas-profiling-7137683fe47f>`_ **[Requires module installation]**
* `A Data Science Project for Beginners (EDA) <https://medium.com/analytics-vidhya/a-data-science-project-for-beginners-exploratory-data-analysis-eda-d334f58f94ee>`_
* `EDA: A Pracitcal Guide and Template for Structured Data <https://towardsdatascience.com/exploratory-data-analysis-eda-a-practical-guide-and-template-for-structured-data-abfbf3ee3bd9>`_
* `EDAâ€”Don't ask how, ask what <https://medium.com/towards-artificial-intelligence/exploratory-data-analysis-eda-dont-ask-how-ask-what-2e29703fb24a>`_ (Part 1)
* `Preparing your Dataset for Modeling â€“Â Quickly and Easily <https://medium.com/towards-artificial-intelligence/preparing-your-dataset-for-modeling-quickly-and-easily-c8c1b89fdb2e>`_ (Part 2)
* `Handling Missing Data <https://towardsdatascience.com/handling-missing-data-for-a-beginner-6d6f5ea53436>`_
* `Introduction to Exploratory Data Analysis (EDA) <https://medium.com/code-heroku/introduction-to-exploratory-data-analysis-eda-c0257f888676>`_

Exploratory Spatial Data Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


* PySAL

{{Mind-Map}}
PySAL
    libpysal
        weights
            Distance Weights
            Continguity Weights
    explore
        esda
        inequality
        pointpats
        ...
    model
        mgwr
        spglm
        spreg
        ...
    viz
        legendgram
        mapclassify
        splot


* `EDA with Pandas on Kaggle <https://www.kaggle.com/kashnitsky/topic-1-exploratory-data-analysis-with-pandas>`_
* `EDA Visualisation using Pandas <https://towardsdatascience.com/exploratory-data-analysis-eda-visualization-using-pandas-ca5a04271607>`_
* `Python EDA Analysis Tutorial <https://www.datacamp.com/community/tutorials/exploratory-data-analysis-python>`_
* `Better EDA with Pandas Profiling <https://towardsdatascience.com/a-better-eda-with-pandas-profiling-e842a00e1136>`_ **[Requires module installation]**
* `Visualising Missing Data <https://geopandas.org/mapping.html##missing-data>`_
* `Choosing Map Colours <https://geopandas.org/mapping.html##choosing-colors>`_

Readings
^^^^^^^^

Dignazio 2020
~~~~~~~~~~~~~

[@dignazio:2020, chap. 6] **The Numbers Don't Speak for Themselves** in *Data Feminism*\ ; `Pre-review URL <https://bookbook.pubpub.org/pub/6ui5n4vo>`_

Elwood 2017
~~~~~~~~~~~

[@Elwood:2017] `DOI <https://doi.org/10.1080/13658816.2017.1334892>`_

Practical
^^^^^^^^^


* ``isna`` and ``isnull``  : å®Œå…¨ç›¸åŒ
* ``pdf[pdf.id.isna()].index.values`` ï¼š æŸ¥æ‰¾ç©ºå€¼
* ``pdf.longitude`` and ``pdf["longitude"]`` ï¼šä¼¼ä¹ä¹Ÿæ²¡å·®åˆ«

Assessment ##2
--------------

..

   I had interpreted it as doing the biography on just the August listing dataset we were given and then in the questions like â€˜to what extent is the data completeâ€™ I analysed whether this dataset in isolation was a complete picture of the process it claims for us to examine and whether integration of the other datasets would be more beneficial (i.e. time-series analysis for airbnb impact etc)



* The focus here is really on approaching the â€˜assignedâ€™ data with a critical eyeâ€”what are the strengths and weaknesses, what are the ethics, etc. Link it to the readings. While it is not necessary to have run any code/done any analysis, it will undoubtedly be easier for many of you to ground your thinking in concrete examples drawn from the data. Bringing other data sets from the InsideAirbnb site into it may help but might just confuse.
* It is not necessary that you perform any new work in Python for Assessment 2. If you are having trouble making *sense* of the data and feel that it would help to think more 'concretely' then you might want to revisit the summary information and plots that we did for one or more of the columns weâ€™ve not looked at... but that is *only* if you think it will help you to answer the questions in greater detail and with more specificity. I am not asking for you to do anything more with the code than you have already done. You will not get a better mark for having written more/new code.
* The readings have invited you to think about how data is generated, for what purpose(s), who is included/excluded, what is missing, etc., as well as issues of ethics (such as the ethical use of data), and these questions and topics are intended to support a critical engagement with the data. I would start by reading up on how the data was collected and for what purpose, and then use that as a launching point for the rest of the assessment.
* It is not necessary that you look at the other data available on the InsideAirbnb web site but, again, if it helps you think through the issues in greater detail then you are free to reference them in your answer. You will not get a better mark for discussing other data sets in detail.
* Please use the template to answer the questions, you don't need to write an intro/conclusion/etc. By the time you've finished filling in the Markdown template you'll have something fairly well-organised that can, hopefully, be read as a kind of highly-structured essay.

Week 7 : Textual Data
---------------------

Lectures
^^^^^^^^

Notebook as Documents
~~~~~~~~~~~~~~~~~~~~~

Not Just for Code
"""""""""""""""""

..

   Markdown and Notebooks can be used for a lot more than just code! In conjunction with Pandoc and/or LaTeX they become platforms for publication.


Recall: Tangled Workflows
"""""""""""""""""""""""""

It's not *just* about mixing code and comment, we *also* want:


* To separate *content* from *presentation*
* To define *mappings* between presentation styles
* To produce the *best-quality* output for the format chosen


.. image:: http://www.zzzhou.me/images/2020/11/25/126z19ld.jpg
   :target: http://www.zzzhou.me/images/2020/11/25/126z19ld.jpg
   :alt: 


^ Examples of this include CSS for web sites, LaTeX templates, and Markdown styles.
^ `MVC approach <https://hackernoon.com/beginners-guide-to-ruby-on-rails-mvc-model-view-controller-pattern-4z19196a>`_ to software design.

Pandoc
""""""

Tool for converting documents *between* formats, including:


* Plain Text/YAML
* Markdown
* LaTeX/PDF
* HTML/Reveal.js
* Jupyter Notebook
* XML/ODT/DOCX
* EPUB/DocBook
* BibTeX/JSON

LaTeX
"""""

Intended for `type-setting of scientific documents <https://www.latex-project.org/about/>`_\ , but has been used for slides, posters, CVs, etc. It is *not* a word processor, it's more like a *compiler*.


.. image:: http://www.zzzhou.me/images/2020/11/25/LaTeX-1.png
   :target: http://www.zzzhou.me/images/2020/11/25/LaTeX-1.png
   :alt: LaTeX-1



.. image:: http://www.zzzhou.me/images/2020/11/25/LaTeX-2.png
   :target: http://www.zzzhou.me/images/2020/11/25/LaTeX-2.png
   :alt: LaTeX-2


^ This format is based on Edward Tufte's VSQD and can be found onHub](https://tufte-latex.github.io/tufte-latex/).

LaTeX in Practice
#################

You  write LaTeX in any text editor, but specialist apps like `Texpad <https://www.texpad.com/>`_ or `Overleaf <https://www.overleaf.com/>`_ make it easier.

.. code-block:: latex

   \documentclass[11pt,article,oneside]{memoir}
   \newcommand{\bl}{\textsc{bl}~\/}
   \usepackage{tabularx}

   \begin{document}
   \maketitle 

   This report provides an overview of activities ...

   \section{Applications}
   A primary objective was the submission...

^ UCL has an institutional license for Overleaf. 
^ This document is then *compiled* (or 'typeset') with the commands provided by the preamble being interpreted and applied. Depending on the length of the document and sophistication of the styles it can take up to 3 or 4 minutes for a book-length document, but small documents should compile in a few seconds.

^ Compilation allows us to do things like have Master Documents that actually work, include PDFs, make forwards and backwards references.

BibTeX
######

Provides bilbiographic support for LaTeX but widely used by other utilities as is *also* plain-text.

.. code-block:: bibtex

   @article{Lavin:2019,
           Author = {Lavin, Matthew J.},
           Doi = {10.46430/phen0082},
           Journal = {The Programming Historian},
           Number = {8},
           Title = {Analyzing Documents with TF-IDF},
           Year = {2019},
           Bdsk-Url-1 = {https://doi.org/10.46430/phen0082}}

   @incollection{Kitchin:2016,
           Author = {Kitchin, R. and Lauriault, T.P. and McArdie, G.},
           Booktitle = {Smart Urbanism},
           Chapter = {2},
           Editor = {Marvin, Luque-Ayala, McFarlane},
           Title = {Smart Cities and the Politics of Urban Data},
           Year = {2016}}

BibTeX in Practice
##################

To reference a document we then need to tell LaTeX or Pandoc where to look:

.. code-block:: latex

   \bibliographystyle{apacite} ## LaTeX
   \bibliography{Spatial_Data_Chapter.bib} ## LaTeX

With citations following formats like:

.. code-block:: latex

   \citep[p.22]{Reades2018} ## LaTeX

Or: 

.. code-block:: markdown

   [@dignazio:2020, chap. 4] ## Markdown

Reveal.js
#########

JavaScript-based presentation framework. Can use Markdown (using the separator ``---`` to separate slides) to generate portable interactive slides including references/bibliographies.

Compare:


* https://github.com/darribas/wmn/blob/master/src/slides/lecture_01.md
* https://github.com/darribas/wmn/blob/master/src/slidedecks/lecture_01.html
* http://darribas.org/wmn/slidedecks/lecture_01.html##/

From Markdown...
################

.. code-block:: bash

   pandoc Syllabus.md \
     -H ./bib/head.tex \
     -H ./bib/chapter.tex \
     -H ./bib/refs.tex \
     -V documentclass=memoir \
     --pdf-engine=xelatex \
     --filter=pandoc-citeproc \
     --metadata-file=metadata.yml \
     --highlight-style=pygments \
     -o Syllabus.pdf

To PDF!
#######


.. image:: http://www.zzzhou.me/images/2020/11/25/Syllabus.png
   :target: http://www.zzzhou.me/images/2020/11/25/Syllabus.png
   :alt: Syllabus


Pros
####


* Simplicity (while writing)
* Flexibility of Form
* Version Control
* High-Quality Outputs
* Enforced Structure

Cons
####


* Complexity (when formatting)
* Collaboration (can be harder)

^ If your document lacks structure (headings, sub-headings, etc.) then it's *impossible* to get a good-looking document. 

^ Conversely, *because* LaTeX and Markdown force you to add structure they ensure that things like ToCs, ToFs, and Bibliographies will generally work 'as advertised'.

Recap of Formatting
"""""""""""""""""""

Headings
########

.. list-table::
   :header-rows: 1

   * - Markdown
     - LaTeX
   * - ``## Heading Level 1``
     - ``\section{Heading Level 1}``
   * - ``### Heading Level 2``
     - ``\subsection{Heading Level 2}``
   * - ``#### Heading Level 3``
     - ``\subsubsection{Heading Level 3}``


Inline Elements
###############

.. list-table::
   :header-rows: 1

   * - Markdown
     - LaTeX
   * - ``1. Numbered item 1``
     - ``\begin{enumerate} \n \item ... \end{enumerate}``
   * - ``- Bulleted list item 1``
     - ``\begin{itemize} \n \item ... \n \end{itemize}``
   * - ``_italics_`` or ``*italics*``
     - ``\emph{italics}`` or ``\textit{italics}``
   * - ``**bold**``
     - ``\textbf{bold}``
   * - ``> blockquote``
     - ``\begin{quote} \n blockquote \end{quote}``
   * - ``Some `code` is here``
     - ``Some \texttt{code} is here``
   * - ``[Link Text](URL)``
     - ``\href{Link Text}{URL}``
   * - ``![Alt Text](Image URL)``
     - ``\begin{figure}\n \includegraphics[opts]{...}\n \end{figure}``


Mathematics
###########

.. list-table::
   :header-rows: 1

   * - Markdown
     - LaTeX
   * - Same as LaTex with 2 $'s
     - :math:`x=5`
   * - Same as LaTex with 2 $'s
     - :math:`\pi`
   * - Same as LaTex with 2 $'s
     - :math:`e = mc^{2}`


We can show all this directly *in* the Notebook!
$$\pi$$; $$e = mc^{2}$$; $$\int\ *{0}^{\inf} x^2 \,dx$$; $$\sum*\ {n=1}^{\infty} 2^{-n} = 1$$

Overleaf `has good documentation <https://www.overleaf.com/learn/latex/Main_Page>`_ for most (basic) applications.

Recap
"""""

..

   You will usually want to Google most things to do with laying out LaTeX code.


Resources
"""""""""


* `Jupyter Tips and Tricks <https://www.kaggle.com/tientd95/jupyter-notebook-tricks>`_
* `Pandoc Demos <https://pandoc.org/demos.html>`_
* `Beginner's Guide to Jupyter Notebooks <https://towardsdatascience.com/beginners-guide-to-jupyter-notebook-8bb85b85085>`_
* `7 Essential Tips to Writing With Jupyter Notebooks <https://towardsdatascience.com/7-essential-tips-for-writing-with-jupyter-notebook-60972a1a8901##699a>`_
* `Version Control with Jupyter <https://towardsdatascience.com/version-control-with-jupyter-notebook-b9630bc5996e>`_
* `Sustainable Publishing using Pandoc and Markdown <https://programminghistorian.org/en/lessons/sustainable-authorship-in-plain-text-using-pandoc-and-markdown>`_

Patterns in Text
~~~~~~~~~~~~~~~~

æ­£åˆ™èƒ½åŒ¹é…ä¸­æ–‡å—

æ­£åˆ™çš„å±€é™æ€§

.. code-block:: python

   >>> '123foo456'.index('foo')
   2
   >>> '123foo456'.split('foo')
   ['123', '456']
   >>> ' 123 foo 456 '.strip() ### ç§»é™¤é¦–ä½ç©ºæ ¼
   '123 foo 456'
   >>> 'HOW NOW BROWN COW?'.lower()
   'how now brown cow?'
   >>> 'How now brown cow?'.replace('brown ','green-')
   'How now green-cow?'

Handling text
"""""""""""""

Regexes are a way for talking *about* patterns observed in text, although their origins are rooted in philosophy and linguistics.

Implemented in Python as:

.. code-block:: python

   import re
   ## re.search(<regex>, <str>)
   s = '123foo456'
   if re.search('123',s):
       print("Found a match.")
   else:
       print("No match.")

Prints ``'Found a match.'``

Capturing Matches
#################

.. code-block:: python

   m = re.search('123',s)
   print(m.start()) ## 0
   print(m.end())## 3
   print(m.span())## (0,3)
   print(m.group())## 123

^ So, we have ``None`` if a search fails, but if it succeeds then we have attributes of the ``match`` objection like start, end, span, and group (this last is going to be particularly interesting since it tells us what matched).

Configuring Matches
###################

.. code-block:: python

   m = re.search('FOO',s)
   print(m) ## None
   m = re.search('FOO',s,re.IGNORECASE)
   print(m) ## <re.Match object; span=(3, 6), match='foo'>

The third parameter allows us to: match newlines (\ ``re.DOTALL``\ ), ignore case (\ ``re.IGNORECASE``\ ), take language into account (\ ``re.LOCALE``\ ), match across lines (\ ``re.MULTILINE``\ ), and write patterns across multiple lines (\ ``re.VERBOSE``\ ). If you need multiple options it's ``re.DOTALL | re.IGNORECASE``. Bitwise again!

More Than One Match
###################

.. code-block:: python

   s = '123foo456foo789'
   lst = re.findall('foo',s)
   print(lst) ## ['foo','foo']
   lst = re.finditer('foo',s)
   [x for x in lst] ## ## [<re.Match object; span=(3, 6), match='foo'>, <re.Match object; span=(9, 12), match='foo'>]
   rs  = re.sub('foo',' ',s)
   print(rs)  ## '123 456 789'
   rs  = re.split(' ',rs)
   print(rs) ## ['123', '456', '789']

Regular Expression
""""""""""""""""""

.. code-block:: python

   >>> import re
   >>> m = re.search('\$((\d+,){2,}\d+)',
           "'That will be $1,000,000 he said...'")
   >>> m.group(1)
   '1,000,000'

This is not even scratching the surface, but it allows to look for sequences of 1-or-more digits followed by a comma... and for those sequence to repeat two or more times, ending with a sequence of digits. The rest is ignored.

Character Classes
#################

.. list-table::
   :header-rows: 1

   * - Characters
     - Regex Meta Class Options
     - 'Antonyms'
   * - a...z
     - ``[a-z]``\ , ``\w`` (word-like characters)
     - ``[^a-z]``\ , ``\W``
   * - A...Z
     - ``[A-Z]``\ , ``\w`` (word-like characters)
     - ``[^A-Z]``\ , ``\W``
   * - 0...9
     - ``[0-9]``\ , ``\d`` (digits)
     - ``[^0-9]``\ , ``\D``
   * - ``' '``\ , ``\n``\ , ``\t``\ , ``\r``\ , ``\f``\ , ``\v``
     - ``\s``
     - ``\S``
   * - ``.``\ , ``[``\ , ``]``\ , ``+``\ , ``$``\ , ``^``\ , ``|``\ , ``{``\ , ``}``\ , ``*``\ , ``(``\ , ``)``\ , ``?``
     - For safety always precede character with a ``\``.
     - None


*Note:*  ``\w`` will include ``_``. And ``\`` is, once again, important as it 'escapes' various characters, and options.

Options
#######

.. list-table::
   :header-rows: 1

   * - Options
     - Regex Meta Class Options
   * - **i**
     - ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…, å®ƒæŠŠå­—æ¯ A åˆ° Z è§†ä¸ºç­‰åŒäºå®ƒä»¬çš„å°å†™å‰¯æœ¬.
   * - **m**
     - å¤šè¡Œ. æŠŠ\ *Haystack*\ è§†ä¸ºè®¸å¤šå•ç‹¬çš„è¡Œï¼ˆå¦‚æœå®ƒåŒ…å«æ–°è¡Œç¬¦ï¼‰çš„é›†åˆè€Œä¸æ˜¯ä¸€ä¸ªå•ä¸ªçš„è¿ç»­è¡Œã€‚å…·ä½“åœ°, å®ƒä¼šæ”¹å˜ä¸‹åˆ—æ–¹å¼:1) æŠ‘æ‰¬ç¬¦ (^) èƒ½åŒ¹é…ç´§è·Ÿåœ¨å†…éƒ¨æ‰€æœ‰æ–°è¡Œç¬¦ä¹‹åçš„ä½ç½®, å¦‚åŒå®ƒæ€»èƒ½åŒ¹é… *Haystack* çš„å¼€å§‹å¤„ä¸€æ · (ä½†å®ƒä¸ä¼šåŒ¹é… *Haystack* çš„ *æœ€åé¢* çš„æ–°è¡Œç¬¦ä¹‹åçš„ä½ç½®).2) ç¾å…ƒç¬¦ ($) èƒ½åŒ¹é… *Haystack* ä¸­ä»»ä½•æ–°è¡Œç¬¦ä¹‹å‰çš„ä½ç½® (å¦‚åŒå®ƒæ€»èƒ½åŒ¹é…æœ€åé¢çš„ä½ç½®).ä¾‹å¦‚ï¼Œæ¨¡å¼â€œm)^abc$â€ä¸­åŒ…å«äº†â€œmâ€é€‰é¡¹æ‰èƒ½åœ¨ *Haystack*\ â€œxyz\ ``r``\ nabcâ€ä¸­å½¢æˆåŒ¹é…ã€‚ä½¿ç”¨äº† "m" é€‰é¡¹æ—¶ "D" é€‰é¡¹ä¼šè¢«å¿½ç•¥.
   * - **s**
     - DotAll. æ­¤é€‰é¡¹ä¼šè®©å¥ç‚¹ (.) åŒ¹é…åŒ…å«æ–°è¡Œç¬¦åœ¨å†…çš„æ‰€æœ‰å­—ç¬¦ (ä¸€èˆ¬æƒ…å†µä¸‹, å®ƒä¸èƒ½åŒ¹é…æ–°è¡Œç¬¦). ç„¶è€Œ, å¦‚æœæ¢è¡Œç¬¦æ˜¯é»˜è®¤çš„ CRLF (\ ``r``\ n), åˆ™å¿…é¡»ä½¿ç”¨ä¸¤ä¸ªå¥ç‚¹æ‰èƒ½è¿›è¡ŒåŒ¹é… (ä¸æ˜¯ä¸€ä¸ª). ä¸è®ºæ˜¯å¦ä½¿ç”¨æ­¤é€‰é¡¹, æ’é™¤å‹å­—ç¬¦ç±» (ä¾‹å¦‚ [^a]) æ€»èƒ½åŒ¹é…æ–°è¡Œç¬¦.
   * - **x**
     - å¿½ç•¥æ¨¡å¼ä¸­çš„ç©ºç™½å­—ç¬¦, é™¤éå¯¹å®ƒä»¬è¿›è¡Œè½¬ä¹‰æˆ–å‡ºç°åœ¨å­—ç¬¦ç±»ä¸­. å­—ç¬¦ ``n å’Œ``\ t åœ¨å®ƒä»¬è¾¾åˆ° PCRE æ—¶ä¼šè¢«å¿½ç•¥, å› ä¸ºå®ƒä»¬å·²ç»æ˜¯åŸå§‹çš„/åŸä¹‰çš„ç©ºç™½å­—ç¬¦ (ä¸ä¹‹ç›¸æ¯”, \n å’Œ \t åˆ™ä¸ä¼šè¢«å¿½ç•¥, å› ä¸ºå®ƒä»¬æ˜¯ PCRE çš„è½¬ä¹‰åºåˆ—). **x** é€‰é¡¹è¿˜ä¼šå¿½ç•¥å­—ç¬¦ç±»å¤–é¢çš„éè½¬ä¹‰ ## å’Œä¸‹ä¸€ä¸ªæ–°è¡Œç¬¦ä¹‹é—´çš„å­—ç¬¦ (åŒ…æ‹¬å®ƒä»¬). è¿™ä½¿å¾—åœ¨å¤æ‚çš„æ¨¡å¼ä¸­æ·»åŠ æ³¨é‡Šæˆä¸ºå¯èƒ½. ç„¶è€Œ, è¿™åªé€‚ç”¨äºæ•°æ®å­—ç¬¦; ç©ºç™½å­—ç¬¦å¯èƒ½æ°¸è¿œéƒ½ä¸ä¼šå‡ºç°åœ¨ç‰¹æ®Šå­—ç¬¦åºåˆ—ä¸­, ä¾‹å¦‚ (?(, å®ƒä»¥æ¡ä»¶å­æ¨¡å¼å¼€å¤´.
   * - **A**
     - å¼ºåˆ¶å›ºå®šåŒ¹é…æ¨¡å¼; å³å®ƒåªèƒ½åŒ¹é… *Haystack* çš„å¼€å§‹å¤„ (å³ä½¿å¼€å§‹å¤„æ˜¯æ¢è¡Œç¬¦, ä¹Ÿä¼šä»æ¢è¡Œç¬¦å¼€å§‹åŒ¹é…è€Œä¸ä»æ¢è¡Œç¬¦ä¹‹åçš„å­—ç¬¦å¼€å§‹). åœ¨å¤§å¤šæ•°æ¡ä»¶ä¸‹, å®ƒçš„ä½œç”¨ç­‰åŒäºåœ¨æ¨¡å¼ä¸­ä½¿ç”¨ "^".
   * - **D**
     - å¼ºåˆ¶ç¾å…ƒç¬¦ ($) åŒ¹é… *Haystack* çš„æœ«ç«¯, å³ä½¿ *Haystack* çš„æœ€åçš„å­—ç¬¦æ˜¯æ–°è¡Œç¬¦. å¦‚æœæ²¡æœ‰æ­¤é€‰é¡¹ï¼Œåˆ™$ä¼šåŒ¹é…æœ€åçš„æ–°è¡Œç¬¦ä¹‹å‰çš„ä½ç½®ï¼ˆå¦‚æœæœ‰æ–°è¡Œç¬¦ï¼Œæ­¤æ—¶åŒ¹é…ä¸ä¼šåŒ…æ‹¬æ–°è¡Œç¬¦ï¼‰ã€‚æ³¨: ä½¿ç”¨äº† "m" é€‰é¡¹æ—¶æ­¤é€‰é¡¹ä¼šè¢«å¿½ç•¥.
   * - **J**
     - å…è®¸é‡å¤ `å‘½åå­æ¨¡å¼ <https://ahkcn.github.io/docs/commands/RegExMatch.htm##NamedSubPat>`_. å®ƒå¯ç”¨äºåœ¨ä¸€ç»„ç›¸åŒçš„å‘½åå­æ¨¡å¼ä¸­åªæœ‰å…¶ä¸­ä¸€ä¸ªå½¢æˆåŒ¹é…çš„æ¨¡å¼. æ³¨: å¦‚æœæœ‰å¤šä¸ªç‰¹æ®Šåå­æ¨¡å¼çš„å®ä¾‹å½¢æˆåŒ¹é…, é‚£ä¹ˆåªä¿å­˜æœ€å·¦è¾¹çš„é‚£ä¸ª. æ­¤å¤–, å˜é‡åä¸åŒºåˆ†å¤§å°å†™.
   * - **U**
     - éè´ªå©ª. è®©é™å®šç¬¦ *+?{} åœ¨å½¢æˆåŒ¹é…æ—¶åªæ¶ˆè€—å¿…éœ€çš„é‚£äº›å­—ç¬¦, æŠŠå‰©ä¸‹çš„éƒ¨åˆ†ç•™ç»™æ¨¡å¼çš„åé¢éƒ¨åˆ†. æ²¡æœ‰ä½¿ç”¨ "U" é€‰é¡¹æ—¶, å¯ä»¥åœ¨è¿™äº›å­—ç¬¦ååŠ ä¸Šé—®å·æ¥é™å®šå®ƒä»¬ä¸ºéè´ªå©ªçš„. ç›¸ååœ°, *\ ä½¿ç”¨äº†* "U" é€‰é¡¹æ—¶, é—®å·ä¼šæˆä¸ºè´ªå©ªåŒ¹é…çš„é™å®šç¬¦.
   * - **X**
     - PCRE_EXTRA. å¯ç”¨ä¸å…¼å®¹ Perl çš„ PCRE åŠŸèƒ½. ç›®å‰, è¿™æ ·çš„å”¯ä¸€åŠŸèƒ½æ˜¯åœ¨æ¨¡å¼ä¸­çš„ä»»æ„åæ–œçº¿åè·Ÿç€æ²¡æœ‰ç‰¹æ®Šå«ä¹‰çš„å­—æ¯æ—¶ä¼šå¯¼è‡´åŒ¹é…å¤±è´¥å¹¶å› æ­¤è®¾ç½® ErrorLevel. æ­¤é€‰é¡¹ä¼šå¸®åŠ©ä¿ç•™æœªä½¿ç”¨çš„åæ–œçº¿åºåˆ—ä¾›å°†æ¥ä½¿ç”¨. å¦‚æœæ²¡æœ‰æ­¤é€‰é¡¹, åæ–œçº¿åè·Ÿç€æ²¡æœ‰ç‰¹æ®Šå«ä¹‰çš„å­—æ¯æ—¶ä¼šè¢«è§†ä¸ºåŸä¹‰çš„ (å³ \g å’Œ g éƒ½è¢«è¯†åˆ«ä¸ºåŸä¹‰çš„ g). ä¸è®ºæ˜¯å¦ä½¿ç”¨æ­¤é€‰é¡¹, æ²¡æœ‰ç‰¹æ®Šå«ä¹‰çš„éå­—æ¯åæ–œçº¿åºåˆ—æ€»æ˜¯è¢«è§†ä¸ºåŸä¹‰çš„ (å³ \/ å’Œ / éƒ½è¢«è§†ä¸ºæ­£æ–œæ ).
   * - **P**
     - ä½ç½®æ¨¡å¼. è¿™ä¼šä½¿ RegExMatch() äº§ç”ŸåŒ¹é…å’Œå…¶å­æ¨¡å¼çš„ä½ç½®å’Œé•¿åº¦è€Œä¸æ˜¯åŒ¹é…å®ƒä»¬çš„å­å­—ç¬¦ä¸². æ›´å¤šç»†èŠ‚è¯·å‚é˜… `UnquotedOutputVar <https://ahkcn.github.io/docs/commands/RegExMatch.htm##PosMode>`_\ ã€‚
   * - **S**
     - ç ”ç©¶æ¨¡å¼æ¥æé«˜æ€§èƒ½. å®ƒå¯ç”¨äºè¦æ‰§è¡Œå¤šæ¬¡çš„ç‰¹æ®Šæ¨¡å¼ (å°¤å…¶æ˜¯å¤æ‚çš„æ¨¡å¼). å¦‚æœ PCRE æ‰¾åˆ°äº†æé«˜æ€§èƒ½çš„æ–¹æ³•, åˆ™ä¼šæŠŠè¿™ä¸ªå‘ç°å‚¨å­˜åˆ°ç¼“å­˜ä¸­æ¨¡å¼çš„æ—è¾¹, ä»¥ä¾¿åœ¨ä¹‹åæ‰§è¡Œç›¸åŒæ¨¡å¼æ—¶ä½¿ç”¨ (åç»­ä½¿ç”¨æ­¤æ¨¡å¼æ—¶è¿˜éœ€è¦æŒ‡å®š S é€‰é¡¹, å› ä¸ºè¦æ‰¾åˆ°ç¼“å­˜ä¸­ç›¸åŒçš„æ¨¡å¼åˆ™å®ƒä»¬çš„é€‰é¡¹ä¹Ÿå¿…é¡»å®Œå…¨ç›¸åŒ, åŒ…æ‹¬å®ƒä»¬çš„é¡ºåº). (è¿™é‡Œçš„ç ”ç©¶ä¸»è¦æŒ‡åœ¨è¿›è¡ŒåŒ¹é…å‰ä½¿ç”¨å…¶ä»–ä¸€äº›é€šå¸¸è¾ƒç®€å•å¿«é€Ÿçš„æ–¹æ³•è¿›è¡Œåˆ¤æ–­, ä¾‹å¦‚å‡è®¾æ¨¡å¼è‡³å°‘åŒ¹é… 5 ä¸ªå­—ç¬¦, è€Œæºå­—ç¬¦ä¸²åªæœ‰ 3 ä¸ª, é‚£ä¹ˆæ­£åˆ™è¡¨è¾¾å¼å¼•æ“ä¼šç›´æ¥è¿”å› "æ²¡æœ‰åŒ¹é…" çš„ç»“æœ, è€Œä¸ä¼šè¿›è¡ŒåŒ¹é….)
   * - **C**
     - å¯ç”¨è‡ªåŠ¨è°ƒå‡ºæ¨¡å¼ã€‚è¯·å‚é˜…\ `æ­£åˆ™è¡¨è¾¾å¼è°ƒå‡º <https://ahkcn.github.io/docs/misc/RegExCallout.htm##auto>`_\ äº†è§£æ›´å¤šä¿¡æ¯ã€‚
   * - **`n**
     - ä»é»˜è®¤çš„æ–°è¡Œç¬¦ (\ ``r``\ n) åˆ‡æ¢åˆ°å•ç‹¬çš„æ¢è¡Œç¬¦ (`n), è¿™æ˜¯ UNIX ç³»ç»Ÿçš„æ ‡å‡†. æ‰€é€‰æ‹©çš„æ–°è¡Œç¬¦ä¼šå½±å“ `é”š (^ å’Œ $) <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##anchor>`_ å’Œ `å«å¥ç‚¹çš„æ¨¡å¼ <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##dot>`_.
   * - **`r**
     - ä»é»˜è®¤çš„æ–°è¡Œç¬¦ (\ ``r``\ n) åˆ‡æ¢åˆ°å•ç‹¬çš„å›è½¦ç¬¦ (`r).
   * - **`a**
     - åœ¨ v1.0.46.06+, ``a å¯ä»¥è¯†åˆ«ä»»æ„ç±»å‹çš„æ–°è¡Œç¬¦, å³``\ r, ``n,``\ r\ ``n,``\ v/VT/vertical tab/chr(0xB), ``f/FF/formfeed/chr(0xC) ä»¥åŠ NEL/next-line/chr(0x85). åœ¨ v1.0.47.05+ï¼Œæ–°è¡Œç¬¦å¯ä»¥è¢«é™åˆ¶ä¸º CRã€LF å’Œ CRLF ä¸‰ç§ï¼Œåªéœ€è¦åœ¨æ¨¡å¼çš„å¼€å§‹å¤„ï¼ˆé€‰é¡¹åé¢ï¼‰æŒ‡å®šå¤§å†™çš„ï¼ˆ*ANYCRLFï¼‰ï¼›ä¾‹å¦‚``\ im)(*ANYCRLF)^abc$`ã€‚


Metacharacters
##############

.. list-table::
   :header-rows: 1

   * - Metacharacter
     - Meaning
   * - .
     - Any character at all
   * - ^
     - Start of a string/line
   * - $
     - End of a string/line
   * - *
     - 0 or more of something
   * - +
     - 1 or more of something
   * - ?
     - 0 or 1 of something; also lazy modifier
   * - {m,n}
     - Between m and n of something
   * - [ ]
     - A set of character literals
   * - ( )
     - Group/remember this sequence of characters
   * - |
     - Or


.. list-table::
   :header-rows: 1

   * - Metacharacter
     - Meaning
   * - **.**
     - é»˜è®¤æƒ…å†µä¸‹, å¥ç‚¹åŒ¹é…é™¤æ–°è¡Œç¬¦ (\ ``r``\ n) åºåˆ—å¤–çš„ä»»ä½•å•ä¸ªå­—ç¬¦, ä½†æ˜¯è¿™ç§ç‰¹æ€§å¯ä»¥ä½¿ç”¨ `DotAll (s) <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##opt_s>`_\ , `æ–°è¡Œ (`n) <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##opt_esc_n>`_\ , `å›è½¦ (`r) <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##opt_esc_r>`_\ , ``a æˆ– (*ANYCRLF) <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##NEWLINE_ANY>`_ é€‰é¡¹è¿›è¡Œæ”¹å˜. ä¾‹å¦‚, **ab.** å¯ä»¥åŒ¹é… abc å’Œ abz ä»¥åŠ ab_.
   * - *****
     - æ˜Ÿå·åŒ¹é…é›¶ä¸ªæˆ–å¤šä¸ªå‰é¢çš„å­—ç¬¦, `å­—ç¬¦ç±» <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##class>`_ æˆ– `å­æ¨¡å¼ <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##subpat>`_. ä¾‹å¦‚, **a*** å¯ä»¥åŒ¹é… ab å’Œ aaab. å®ƒè¿˜å¯ä»¥åŒ¹é…å®Œå…¨ä¸åŒ…å« "a" çš„ä»»æ„å­—ç¬¦ä¸²çš„å¼€å§‹å¤„.\ **é€šé…ç¬¦:** å¥ç‚¹æ˜Ÿå·æ¨¡å¼ **.*** æ˜¯åŒ¹é…èŒƒå›´æœ€å¹¿çš„æ¨¡å¼ä¹‹ä¸€, å› ä¸ºå®ƒå¯ä»¥åŒ¹é…é›¶ä¸ªæˆ–å¤šä¸ª *ä»»æ„* å­—ç¬¦ (é™¤äº†æ–°è¡Œç¬¦: ``r å’Œ``\ n). ä¾‹å¦‚, **abc.*123** å¯ä»¥åŒ¹é… abcAnything123, ä¹Ÿèƒ½åŒ¹é… abc123.
   * - **?**
     - é—®å·åŒ¹é…é›¶æˆ–ä¸€ä¸ªå‰é¢çš„å­—ç¬¦, `å­—ç¬¦ç±» <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##set>`_ æˆ– `å­æ¨¡å¼ <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##subpat>`_. å¯ä»¥ç†è§£ä¸º "å‰é¢çš„é‚£é¡¹æ˜¯å¯é€‰çš„". ä¾‹å¦‚, **colou?r** å¯ä»¥åŒ¹é… color å’Œ colour, å› ä¸º "u" æ˜¯å¯é€‰çš„.
   * - **+**
     - åŠ å·åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªå‰é¢çš„å­—ç¬¦, `å­—ç¬¦ç±» <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##class>`_ æˆ– `å­æ¨¡å¼ <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##subpat>`_. ä¾‹å¦‚ **a+** å¯ä»¥åŒ¹é… ab å’Œ aaab. ä½†ä¸ **a*** å’Œ **a?** ä¸åŒçš„æ˜¯, æ¨¡å¼ **a+** ä¸ä¼šåŒ¹é…å¼€å§‹å¤„æ²¡æœ‰ "a" çš„å­—ç¬¦ä¸².
   * - {min,max}
     - åŒ¹é…å‡ºç°æ¬¡æ•°ä»‹äº *min* å’Œ *max* çš„å‰é¢çš„å­—ç¬¦, `å­—ç¬¦ç±» <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##set>`_ æˆ– `å­æ¨¡å¼ <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##subpat>`_. ä¾‹å¦‚, **a{1,2}** å¯ä»¥åŒ¹é… ab ä½†åªåŒ¹é… aaab ä¸­çš„å‰ä¸¤ä¸ª a.æ­¤å¤–, {3} è¡¨ç¤ºå‡†ç¡®åŒ¹é… 3 æ¬¡, è€Œ {3\ **,**\ } åˆ™è¡¨ç¤ºåŒ¹é… 3 æ¬¡æˆ–æ›´å¤š. æ³¨: æŒ‡å®šçš„æ•°å­—å¿…é¡»å°äº 65536, ä¸”ç¬¬ä¸€ä¸ªå¿…é¡»å°äºç­‰äºç¬¬äºŒä¸ª.
   * - **[...]**
     - **å­—ç¬¦ç±»:** æ–¹æ‹¬å·æŠŠä¸€åˆ—å­—ç¬¦æˆ–ä¸€ä¸ªèŒƒå›´æ‹¬åœ¨äº†ä¸€èµ· (æˆ–ä¸¤è€…). ä¾‹å¦‚, **[abc]** è¡¨ç¤º "a, b æˆ– c çš„ä¸­ä»»ä½•ä¸€ä¸ªå­—ç¬¦". ä½¿ç”¨ç ´æŠ˜å·æ¥åˆ›å»ºèŒƒå›´; ä¾‹å¦‚, **[a-z]** è¡¨ç¤º "åœ¨å°å†™å­—æ¯ a å’Œ z (åŒ…å«çš„) ä¹‹é—´çš„ä»»ä½•ä¸€ä¸ªå­—ç¬¦". åˆ—è¡¨å’ŒèŒƒå›´å¯ä»¥ç»„åˆåœ¨ä¸€èµ·; ä¾‹å¦‚ **[a-zA-Z0-9_]** è¡¨ç¤º "å­—æ¯, æ•°å­—æˆ–ä¸‹åˆ’çº¿ä¸­çš„ä»»ä½•ä¸€ä¸ªå­—ç¬¦".å­—ç¬¦ç±»åé¢å¯ä»¥ä½¿ç”¨ *, ?, + æˆ– {min,max} è¿›è¡Œé™å®š. ä¾‹å¦‚, **[0-9]+** åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªä»»æ„æ•°å­—; å› æ­¤å®ƒå¯ä»¥åŒ¹é… xyz123 ä½†ä¸ä¼šåŒ¹é… abcxyz.é€šè¿‡ [[:xxx:]] è¿˜æ”¯æŒä¸‹åˆ— POSIX å‘½åé›†, å…¶ä¸­ xxx æ˜¯ä¸‹åˆ—å•è¯çš„å…¶ä¸­ä¸€ä¸ª: alnum, alpha, ascii (0-127), blank (space æˆ– tab), cntrl (æ§åˆ¶å­—ç¬¦), digit (0-9), xdigit (åå…­è¿›åˆ¶æ•°), print, graph (æ’é™¤äº†ç©ºæ ¼çš„æ‰“å°å­—ç¬¦), punct, lower, upper, space (ç©ºç™½), word (ç­‰åŒäº `\w <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##word>`_\ ).åœ¨å­—ç¬¦ç±»ä¸­, åªæœ‰åœ¨ç±»ä¸­å…·æœ‰ç‰¹æ®Šå«ä¹‰çš„å­—ç¬¦æ‰éœ€è¦è¿›è¡Œè½¬ä¹‰; ä¾‹å¦‚ **[\^a]**\ , **[a-b]**\ , **[a]]** å’Œ **[\a]**.
   * - **[^...]**
     - åŒ¹é… **ä¸** åœ¨ç±»ä¸­çš„ä»»ä½•ä¸€ä¸ªå­—ç¬¦. ä¾‹å¦‚, **[^/]*** åŒ¹é…é›¶ä¸ªæˆ–å¤šä¸ª *ä¸æ˜¯* æ­£æ–œæ çš„ä»»æ„å­—ç¬¦, ä¾‹å¦‚ http://. åŒæ ·åœ°, **[^0-9xyz]** åŒ¹é…æ—¢ä¸æ˜¯æ•°å­—ä¹Ÿä¸æ˜¯ x, y æˆ– z çš„ä»»ä½•ä¸€ä¸ªå­—ç¬¦.
   * - **\d**
     - åŒ¹é…ä»»æ„ä¸€ä¸ªæ•°å­— (ç›¸å½“äºç±» **[0-9]**\ ). ç›¸ååœ°ï¼Œå¤§å†™çš„\Dè¡¨ç¤ºâ€œä»»æ„çš„\ *é*\ æ•°å­—å­—ç¬¦â€ã€‚è¿™ä¸ªå’Œä¸‹é¢çš„ä¸¤ä¸ªéƒ½å¯ä»¥ç”¨åœ¨ `å­—ç¬¦ç±» <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##set>`_ ä¸­; ä¾‹å¦‚, **[\d.-]** è¡¨ç¤º "ä»»ä½•æ•°å­—, å¥ç‚¹æˆ–è´Ÿå·".
   * - **\s**
     - åŒ¹é…ä»»æ„å•ä¸ªç©ºç™½å­—ç¬¦ , ä¸»è¦æ˜¯ç©ºæ ¼, tab å’Œæ–°è¡Œç¬¦ (\ ``r å’Œ``\ n). ç›¸ååœ°, å¤§å†™çš„ \S è¡¨ç¤º "ä»»ä½• *é*\ ç©ºç™½å­—ç¬¦".
   * - **\w**
     - åŒ¹é…ä»»ä½•å•ä¸ª "å•è¯" å­—ç¬¦, å³å­—æ¯, æ•°å­—æˆ–ä¸‹åˆ’çº¿. è¿™ç­‰åŒäº **[a-zA-Z0-9_]**. ç›¸ååœ°, å¤§å†™çš„ \W è¡¨ç¤º "ä»»ä½• *é*\ å•è¯å­—ç¬¦".
   * - **^ $**
     - æŠ‘æ‰¬ç¬¦ (^) å’Œç¾å…ƒç¬¦ ($) è¢«ç§°ä¸º *é”š*\ , å› ä¸ºå®ƒä»¬ä¸æ¶ˆè€—ä»»ä½•å­—ç¬¦; ç›¸ååœ°, å®ƒä»¬æŠŠæ¨¡å¼é™å®šåœ¨è¢«æœç´¢å­—ç¬¦ä¸²çš„å¼€å§‹æˆ–æœ«å°¾è¿›è¡ŒåŒ¹é….åœ¨æ¨¡å¼çš„å¼€å§‹å¤„ä½¿ç”¨ **^** è¡¨ç¤ºéœ€è¦åœ¨è¡Œçš„å¼€å§‹å¤„è¿›è¡ŒåŒ¹é…. ä¾‹å¦‚, **^abc** å¯ä»¥åŒ¹é… abc123 ä½†ä¸åŒ¹é… 123abc.åœ¨æ¨¡å¼çš„æœ«å°¾å¤„ä½¿ç”¨ **$** è¡¨ç¤ºéœ€è¦åœ¨è¡Œçš„æœ«ç«¯è¿›è¡ŒåŒ¹é…. ä¾‹å¦‚, **abc$** å¯ä»¥åŒ¹é… 123abc ä½†ä¸èƒ½åŒ¹é… abc123.è¿™ä¸¤ä¸ªé”šè¿˜å¯ä»¥ç»„åˆä½¿ç”¨. ä¾‹å¦‚, **^abc$** ä»…åŒ¹é… abc (å³åœ¨å®ƒçš„å‰é¢æˆ–åé¢ä¸èƒ½æœ‰å¦å¤–çš„å­—ç¬¦).å¦‚æœè¢«æœç´¢çš„æ–‡æœ¬åŒ…å«å¤šè¡Œ, åˆ™å¯ä»¥ä½¿ç”¨ `"m" é€‰é¡¹ <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##Multiline>`_ è®©é”šåº”ç”¨äºæ¯è¡Œè€Œä¸æ˜¯æŠŠæ‰€æœ‰æ–‡æœ¬ä½œä¸ºæ•´ä½“. ä¾‹å¦‚, **m)^abc$** å¯ä»¥åŒ¹é… 123\ ``r``\ nabc\ ``r``\ n789. ä½†å¦‚æœæ²¡æœ‰ "m" é€‰é¡¹, åˆ™ä¸ä¼šå½¢æˆåŒ¹é….
   * - **\b**
     - \b è¡¨ç¤º "å•è¯è¾¹ç•Œ", å®ƒç±»ä¼¼é”š, å› ä¸ºå®ƒä¸æ¶ˆè€—ä»»ä½•å­—ç¬¦. å®ƒè¦æ±‚å½“å‰å­—ç¬¦çš„ `çŠ¶æ€ä¸ºå•è¯å­—ç¬¦ (\w) <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##word>`_\ , ä¸å‰ä¸€ä¸ªå­—ç¬¦çš„çŠ¶æ€ç›¸å. å®ƒé€šå¸¸ç”¨æ¥é¿å…æ„å¤–åœ°åŒ¹é…åˆ°åœ¨å…¶ä»–å•è¯å†…çš„æŸä¸ªå•è¯. ä¾‹å¦‚, **\bcat\b** ä¸ä¼šåŒ¹é… catfish, ä½†å®ƒå¯ä»¥åŒ¹é…ä¸è®ºå‘¨å›´æ˜¯å¦æœ‰æ ‡ç‚¹æˆ–ç©ºç™½çš„ cat. å¤§å†™çš„ \B åˆ™ç›¸å: å®ƒè¦æ±‚å½“å‰å­—ç¬¦ *ä¸æ˜¯* å•è¯çš„è¾¹ç•Œ.
   * - **|**
     - ç«–çº¿å°†ä¸¤ä¸ªæˆ–å¤šä¸ªå¯é€‰é¡¹ç›®åˆ†éš”å¼€æ¥. å¦‚æœå¯é€‰é¡¹ç›®ä¸­ *ä»»ä½•ä¸€ä¸ª* æ»¡è¶³æ¡ä»¶, åˆ™ä¼šå½¢æˆåŒ¹é…. ä¾‹å¦‚, **gray|grey** æ—¢å¯ä»¥åŒ¹é… gray ä¹Ÿå¯ä»¥åŒ¹é… grey. åŒæ ·åœ°, æ¨¡å¼ **gr(a|e)y** ä¸­é€šè¿‡ä¸‹é¢æè¿°çš„æ‹¬å·çš„å¸®åŠ©å¯ä»¥å®ç°åŒæ ·çš„ä½œç”¨.
   * - **(...)**
     - æ‹¬åœ¨æ‹¬å·ä¸­çš„é¡¹ç›®å¸¸ç”¨äº:ç¡®å®šæ±‚å€¼çš„é¡ºåº. ä¾‹å¦‚, **(Sun|Mon|Tues|Wednes|Thurs|Fri|Satur)day** å¯ä»¥åŒ¹é…ä»»ä½•ä¸€å¤©çš„åç§°.æŠŠ *****\ , **?**\ , **+** æˆ– **{min,max}** åº”ç”¨åˆ° *ç³»åˆ—* å­—ç¬¦è€Œä¸åªæ˜¯å•ä¸ªå­—ç¬¦. ä¾‹å¦‚, **(abc)+** åŒ¹é…ä¸€ä¸ªæˆ–ä¸€ä¸²å­—ç¬¦ä¸² "abc"; å› æ­¤å®ƒå¯ä»¥åŒ¹é… abcabc123 ä½†ä¸ä¼šåŒ¹é… ab123 æˆ– bc123.æ•è·å­æ¨¡å¼, ä¾‹å¦‚ **abc(.*)xyz** ä¸­çš„å¥ç‚¹æ˜Ÿå·. ä¾‹å¦‚, `RegExMatch() <https://ahkcn.github.io/docs/commands/RegExMatch.htm>`_ ä¼šæŠŠåŒ¹é…æ¯ä¸ªå­æ¨¡å¼çš„å­å­—ç¬¦ä¸²ä¿å­˜åˆ° `è¾“å‡ºæ•°ç»„ <https://ahkcn.github.io/docs/commands/RegExMatch.htm##Array>`_. åŒæ ·åœ°, `RegExReplace() <https://ahkcn.github.io/docs/commands/RegExReplace.htm>`_ ä¸­å…è®¸æŠŠåŒ¹é…æ¯ä¸ªå­æ¨¡å¼çš„å­å­—ç¬¦ä¸²é€šè¿‡åƒ $1 è¿™æ ·çš„ `åå‘å¼•ç”¨ <https://ahkcn.github.io/docs/commands/RegExReplace.htm##BackRef>`_ é‡æ–°æ’å…¥åˆ°æ›¿æ¢ç»“æœä¸­. è¦ä½¿ç”¨ä¸æ•è·å­æ¨¡å¼çš„æ‹¬å·, è¯·æŠŠæ‹¬å·å†…çš„å¼€å§‹ä¸¤ä¸ªå­—ç¬¦æŒ‡å®šä¸º **?:**\ ; ä¾‹å¦‚: **(?:.*)**\ åœ¨åŒ¹é…è¿‡ç¨‹ä¸­æ”¹å˜ `é€‰é¡¹ <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##Options>`_. ä¾‹å¦‚, **(?im)** ä¼šä¸ºæ¨¡å¼çš„åç»­éƒ¨åˆ†æ‰“å¼€ä¸åŒºåˆ†å¤§å°å†™å’Œå¤šè¡Œé€‰é¡¹ (å¦‚æœå®ƒåœ¨å­æ¨¡å¼ä¸­åˆ™å®ƒä¼šæ”¹å˜å­æ¨¡å¼çš„é€‰é¡¹). ç›¸ååœ°, **(?-im)** ä¼šå…³é—­å®ƒä»¬. æ”¯æŒé™¤ DPS\ ``r``\ n`a å¤–çš„æ‰€æœ‰é€‰é¡¹.
   * - **\t \r ç­‰ç­‰.**
     - è¿™äº›è½¬ä¹‰åºåˆ—è¡¨ç¤ºç‰¹æ®Šçš„å­—ç¬¦. æœ€å¸¸è§çš„æœ‰ **\t** (tab), **\r** (å›è½¦) å’Œ **\n** (æ¢è¡Œ). åœ¨ AutoHotkey, åœ¨è¿™äº›æƒ…å†µä¸­è¿˜å¯ä»¥ä½¿ç”¨é‡éŸ³ç¬¦ (\ ``) ä»£æ›¿åæ–œçº¿. è¿˜æ”¯æŒ \xhh æ ¼å¼çš„è½¬ä¹‰åºåˆ—, å…¶ä¸­ *hh* æ˜¯ä»‹äº 00 å’Œ FF ä¹‹é—´çš„ä»»æ„ ANSI å­—ç¬¦çš„åå…­è¿›åˆ¶ç .åœ¨ v1.0.46.06+, **\R** è¡¨ç¤º "å•ä¸ªä»»æ„ç±»å‹çš„æ–°è¡Œç¬¦", å³åœ¨ [``\ a é€‰é¡¹](https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##NEWLINE_ANY) ä¸­åˆ—å‡ºçš„è¿™äº› (ç„¶è€Œ, \R åœ¨ `å­—ç¬¦ç±» <https://ahkcn.github.io/docs/misc/RegEx-QuickRef.htm##class>`_ ä¸­ä»…ä»…è¡¨ç¤ºå­—æ¯ "R"). åœ¨ v1.0.47.05+, **\R** å¯ä»¥è¢«é™åˆ¶ä¸º CR, LF å’Œ CRLF ä¸‰ç§, åªéœ€è¦åœ¨æ¨¡å¼çš„å¼€å§‹å¤„ (é€‰é¡¹åé¢) æŒ‡å®šå¤§å†™çš„ (\ *BSR_ANYCRLF) ; ä¾‹å¦‚ **im)(\*\ BSR_ANYCRLF)abc\Rxyz**
   * - **\p{xx} \P{xx} \X**
     - [AHK_L 61+]: Unicode å­—ç¬¦å±æ€§. åœ¨ ANSI ç‰ˆæœ¬ä¸­ä¸æ”¯æŒ. **\p{xx}** åŒ¹é…å¸¦ xx å±æ€§çš„å­—ç¬¦è€Œ **\P{xx}** åŒ¹é… *ä¸å¸¦* xx å±æ€§çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦. ä¾‹å¦‚, **\pL** åŒ¹é…ä»»æ„ä¸€ä¸ªå­—æ¯è€Œ **\p{Lu}** åŒ¹é…ä»»æ„ä¸€ä¸ªå¤§å†™å­—æ¯. **\X** åŒ¹é…ç»„æˆæ‰©å±• Unicode åºåˆ—çš„ä»»ä½•æ•°ç›®çš„å­—ç¬¦.å¯¹äºå—æ”¯æŒçš„å±æ€§åç§°çš„å®Œæ•´åˆ—è¡¨å’Œå…¶ä»–ç»†èŠ‚, è¯·åœ¨ `www.pcre.org/pcre.txt <http://www.pcre.org/pcre.txt>`_ ä¸­æœç´¢ "\p{xx}".
   * - **(*UCP)**
     - [AHK\ *L 61+]: è€ƒè™‘åˆ°æ€§èƒ½, \d, \D, \s, \S, \w, \W, \b å’Œ \B é»˜è®¤æƒ…å†µä¸‹åªè¯†åˆ« ASCII å­—ç¬¦, å³ä½¿åœ¨ Unicode ç‰ˆæœ¬ä¸­ä¹Ÿæ˜¯å¦‚æ­¤. å¦‚æœæ¨¡å¼ä»¥ **(*UCP)** å¼€å¤´, åˆ™ä¼šä½¿ç”¨ Unicode å±æ€§æ¥åˆ¤æ–­å“ªä¸ªå­—ç¬¦åŒ¹é…. ä¾‹å¦‚, \w å˜æˆç›¸å½“äº **[\p{L}\p{N}*\ ]\ ** è€Œ \d å˜æˆç­‰åŒäº **\ \p{Nd}**.


**è´ªå©ª**\ ï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œ\ *****\ ã€\ **?**\ ã€\ **+** å’Œ **{min,max}** æ˜¯è´ªå©ªçš„ï¼Œå› ä¸ºå®ƒä»¬æ¶ˆè€—åˆ°\ *æœ€åä¸€ä¸ª*\ èƒ½æ»¡è¶³æ•´ä¸ªæ¨¡å¼çš„å¯èƒ½çš„æ‰€æœ‰å­—ç¬¦ã€‚è¦è®©å®ƒä»¬åœåœ¨ *é¦–ä¸ª* å¯èƒ½çš„å­—ç¬¦, è¯·åœ¨å®ƒä»¬åé¢åŠ ä¸Šé—®å·. ä¾‹å¦‚, æ¨¡å¼ **<.+>** (å…¶ä¸­æ²¡æœ‰é—®å·) è¡¨ç¤º: "æœç´¢ä¸€ä¸ª <, æ¥ç€ä¸€ä¸ªæˆ–å¤šä¸ªä»»æ„å­—ç¬¦, ç„¶åæ˜¯ä¸€ä¸ª >". è¦åœ¨åŒ¹é… *æ•´ä¸ª* å­—ç¬¦ä¸² **<**\ em>text</em\ **>** æ—¶åœæ­¢, è¯·åœ¨åŠ å·ååŠ ä¸Šé—®å·: **<.+?>**. è¿™æ ·ä¼šè®©åŒ¹é…åœ¨ç¬¬ä¸€ä¸ª '>' å¤„åœæ­¢, å› æ­¤å®ƒåªåŒ¹é…ç¬¬ä¸€ä¸ªæ ‡ç­¾ **<**\ em\ **>**.

**é¢„æµ‹å’Œå›é¡¾æ–­è¨€**\ ï¼šè¿™ç»„ **(?=...)**\ ã€\ **(?!...)**\ ã€\ **(?<=...)** å’Œ **(?<!...)** è¢«ç§°ä¸º\ *æ–­è¨€*\ ï¼Œå› ä¸ºå®ƒä»¬è¦æ±‚ç¬¦åˆæŸä¸ªæ¡ä»¶ä½†ä¸æ¶ˆè€—ä»»ä½•å­—ç¬¦ã€‚ä¾‹å¦‚, **abc(?=.*xyz)** ä¸­å«æœ‰é¢„æµ‹æ–­è¨€, å®ƒè¦æ±‚åœ¨å­—ç¬¦ä¸² abc å³è¾¹çš„æŸä¸ªä½ç½®å­˜åœ¨å­—ç¬¦ä¸² xyz (å¦‚æœä¸å­˜åœ¨, åˆ™åŒ¹é…å¤±è´¥). **(?=...)** è¢«ç§°ä¸º *æ­£* é¢„æµ‹æ–­è¨€, å› ä¸ºå®ƒè¦æ±‚æŒ‡å®šçš„æ¨¡å¼å­˜åœ¨. ç›¸ååœ°, **(?!...)** æ˜¯ *è´Ÿ* é¢„æµ‹æ–­è¨€, å› ä¸ºå®ƒè¦æ±‚æŒ‡å®šçš„æ¨¡å¼ *ä¸* å­˜åœ¨. åŒæ ·åœ°, **(?<=...)** å’Œ **(?<!...)** åˆ†åˆ«æ˜¯æ­£çš„å’Œè´Ÿçš„ *å›é¡¾* æ–­è¨€, å› ä¸ºå®ƒä»¬æ£€æŸ¥å½“å‰ä½ç½®çš„ *å·¦è¾¹* è€Œä¸æ˜¯å³è¾¹. å›é¡¾æ¯”é¢„æµ‹å—åˆ°æ›´å¤šçš„é™åˆ¶, å› ä¸ºå®ƒä»¬ä¸æ”¯æŒå¯å˜å¤§å°çš„é™å®šç¬¦, ä¾‹å¦‚ *****\ , **?** å’Œ **+**. è½¬ä¹‰åºåˆ— \K ç±»ä¼¼äºå›é¡¾æ–­è¨€, å› ä¸ºå®ƒä¼šè®©å‰ä¸€ä¸ªåŒ¹é…çš„å­—ç¬¦åœ¨æœ€åçš„åŒ¹é…å­—ç¬¦ä¸²ä¸­çœç•¥. ä¾‹å¦‚, **foo\Kbar** å¯ä»¥åŒ¹é… "foobar" ä½†æŠ¥å‘ŠåŒ¹é…çš„ç»“æœä¸º "bar".

Building Blocks
###############

.. list-table::
   :header-rows: 1

   * - Regex
     - Interpretation
   * - ``r'\s*'``
     - 0 or more spaces
   * - ``r'\d+'``
     - 1 or more digits
   * - ``r'[A-Fa-f0-7]{5}'``
     - Exactly 5 hexadecimal 'digits'
   * - ``r'\w+\.\d{2,}'``
     - 1 or more 'wordish' characters, followed by a full-stop, then 2 or more digits
   * - ``r'^[^@]+@\w+'``
     - One more non-@ characters at the start of a line, followed by a '@' then 1 or more 'wordish' characters.
   * - ``r'(uk|eu|fr)$'``
     - The characters 'uk' or 'eu' or 'fr' at the end of a line.


Examples
########

.. code-block:: python

   re.match(r'^[^@]+@([a-z0-9\-]+\.){1,5}[a-z0-9\-]+$', s)

   re.match(r'\d{4}-\d{2}-\d{2}', s)

   re.match(r'^\s*$', s)

   re.match(r'^(http|https|ftp):[\/]{2}([a-zA-Z0-9\-]+\.){1,4}[a-zA-Z]{2,5}(:[0-9]+)?\/?([a-zA-Z0-9\-\._\?\'\/\\\+\&\%\$##\=~]*)',s)

   re.match(r'([Gg][Ii][Rr] 0[Aa]{2})|((([A-Za-z][0-9]{1,2})|(([A-Za-z][A-Ha-hJ-Yj-y][0-9]{1,2})|(([A-Za-z][0-9][A-Za-z])|([A-Za-z][A-Ha-hJ-Yj-y][0-9][A-Za-z]?))))\s?[0-9][A-Za-z]{2})',s)

   regex = r"""
   ([GIR] 0[A]{2})|    ## Girobank 
   (
     (
       ([A-Z][0-9]{1,2})| ## e.g A00...Z99
         (
           ([A-Z][A-HJ-Y][0-9]{1,2})|  ## e.g. AB54...ZX11
             (([A-Z][0-9][A-Z])|  ## e.g. A0B...Z9Z 
             ([A-Z][A-HJ-Y][0-9][A-Z]?))  ## e.g. WC1 or WC1H
           )
         )
       \s?[0-9][A-Z]{2} ## e.g. 5RX
     )
   """
   re.match(regex,s,re.VERBOSE|re.IGNORECASE) ## Can also use: re.X|re.I

^ This is the `government's own regex <https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/488478/Bulk_Data_Transfer_-_additional_validation_valid_from_12_November_2015.pdf>`_ but is probably *not* 100% accurate.

Resources
#########


* `Python Documentation <https://docs.python.org/3/howto/regex.html>`_
* `Real Python: Regular Expressions 1 <https://realpython.com/regex-python/>`_
* `Real Python: Regular Expressions 2 <https://realpython.com/regex-python-part-2/>`_
* `Data Camp RegEx Tutorial <https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial>`_
* `Introduction to Regex <https://medium.com/better-programming/introduction-to-regex-8c18abdd4f70>`_
* `Understanding RegExes in Python <https://medium.com/better-programming/introduction-to-regex-8c18abdd4f70>`_
* `Demystifying RegExes in Python <https://medium.com/@snk.nitin/your-guide-to-using-regular-expressions-in-python-a7908b8e4b68>`_
* `Python RegExes <https://medium.com/@devopslearning/python-regular-expression-8ee28d35f3a7>`_
* `Mastering String Methods in Python <https://towardsdatascience.com/mastering-string-methods-in-python-456174ede911>`_

Thanks to `Yogesh Chavan <https://levelup.gitconnected.com/extremely-useful-regular-expression-examples-for-real-world-applications-567e844a0822>`_ and `Nicola Pietroluongo <https://www.sitepoint.com/demystifying-regex-with-practical-examples/>`_ for examples.

Cleaning Text
~~~~~~~~~~~~~

Vectorisation & Parallelisation
"""""""""""""""""""""""""""""""

Pandas.apply() vs. Numpy
########################

Numpy is fully vectorised and will almost *always* out-perform operations like Pandas ``apply``\ , but both are massive improvements on for loops:


* Execute row-wise and column-wise operations.
* Apply any arbitrary function to individual elements or whole axes.
* Can make use of ``lambda`` functions too for 'one off' operations.

.. code-block:: python

   import numpy as np
   df.apply(np.sqrt) ## Square root of all values
   df.apply(np.sum, axis=0) ## Sum by row

Lambda Functions
################

.. code-block:: python

   >>> x = lambda a : a + 10
   >>> print(x(5))
   15

Or:

.. code-block:: python

   >>> full_name = lambda first, last: f'Full name: {first.title()} {last.title()}'
   >>> full_name('guido', 'van rossum')
   'Guido Van Rossum'

Dealing with Structured Text
""""""""""""""""""""""""""""

Beautiful Soup & Selenium
#########################

Two stages to acquiring web-based documents:


#. Accessing the document: ``urllib`` can deal with many issues (even authentication), but *not* with dynamic web pages (which are increasingly common); for that, you need `Selenium <https://selenium-python.readthedocs.io/>`_ (library + driver).
#. Processing the document: simple data can be extracted from web pages with RegularExpressions, but *not* with complex (esp. dynamic) content; for that, you need `BeautifulSoup4 <https://www.crummy.com/software/BeautifulSoup/bs4/doc/>`_.

These interact with wider issues of Fair Use (e.g. rate limits and licenses); processing pipelines (e.g. saving WARCs or just the text file, multiple stages, etc.); and other practical constraints.

Regular Expressions / Breaks
############################

Need to look at how the data is organised:


* For very large corpora, you might want one document at a time (batch).
* For very large files, you might want one line at a time (streaming).
* For large files in large corpora, you might want more than one machine.

^ See the `OpenVirus Project <https://blogs.bl.uk/digital-scholarship/2020/05/searching-etheses-for-the-openvirus-project.html>`_.

Managing Vocabularies
"""""""""""""""""""""

Starting Points
###############

These strategies can be sued singly or all-together:


* Stopwords
* Case
* Accent-stripping
* Punctuation
* Numbers

Sample stopwords:

.. code-block:: python

   {'further', 'her', 'their', 'we', 'just', 'why', 'or', 'each', 's', "it's", 'ma', 'below', 'am', 'more', "couldn't", "should've", 'was', "mightn't", 'weren', 'ourselves', 'have', 'if', 'then', 'from', ...}

But these are just a *starting* point!

^ What's the semantic difference between 1,000,000 and 999,999?

Distributional Pruning
######################

We can prune from both ends of the distribution:


* Overly rare words: what does a word used in *one* document help us to do?
* Overly common ones: what does a word used in *every* document help us to do?

^ Again, no hard-and-fast rules: can be done on raw counts, percentage of all documents, etc. Choices will, realistically, depend on the nature of the data.

Stemming & Lemmatisation
""""""""""""""""""""""""

Why Stem or Lemmatise?
######################

Reduce the breadth of human expression:


* Porter & Snowball Stemming: rules-based truncation to a stem (can be augmented by language awareness).
* Lemmatisation: dictionary-based 'deduplication' to a lemma (can be augmented by POS-tagging).

Compare:

.. list-table::
   :header-rows: 1

   * - Source
     - Porter
     - Snowball
     - Lemmatisation
   * - monkeys
     - monkey
     - monkey
     - monkey
   * - cities
     - citi
     - citi
     - city
   * - complexity
     - complex
     - complex
     - complexity
   * - Reades
     - read
     - read
     - Reades


Resources
"""""""""


* `Vectorisation in Python <https://towardsdatascience.com/python-vectorization-5b882eeef658>`_
* `Lambda Functions <https://www.w3schools.com/python/python_lambda.asp>`_
* `Real Python Lambda Functions <https://realpython.com/python-lambda/>`_
* `Stemming words with NLTK <https://pythonprogramming.net/stemming-nltk-tutorial/>`_
* `Stemming and Lemmatisation in Python <https://www.datacamp.com/community/tutorials/stemming-lemmatization-python>`_
* `KD Nuggets: A Practitioner's Guide to NLP <https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html>`_
* `KD Nuggets: Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Semantics and Pragmatics <https://www.kdnuggets.com/2020/08/linguistic-fundamentals-natural-language-processing.html>`_
* `Roadmap to Natural Language Processing (NLP) <https://www.kdnuggets.com/2020/10/roadmap-natural-language-processing-nlp.html>`_

Analysing Text
~~~~~~~~~~~~~~

One-Hot Encoding
""""""""""""""""

May already be familiar with concept as 'dummy variables' in economics/regression:

.. list-table::
   :header-rows: 1

   * - Document
     - UK
     - Top
     - Pop
     - Coronavirus
   * - News
     - 1
     - 1
     - 0
     - 1
   * - Culture
     - 0
     - 1
     - 1
     - 0
   * - Politics
     - 1
     - 0
     - 0
     - 1
   * - Entertainment
     - 1
     - 1
     - 1
     - 1


^ Certainly One-Hot encoders are rarely, if ever, used this way, but for keyword detection this might be appropriate: i.e. this word was used in this document!

^ Only difference is One Hot == $$n$$ variables, Dummy == $$n-1$$.

^ Definitely some 'gotchas' in deployment: one-hot models shouldn't have an intercept unless you apply a 'ridge shrinkage penalty'. Standardisation affects whether or not an intercept is needed.

The 'Bag of Words'
""""""""""""""""""

Could simply be seen as an extension of binarised approach on preceding slide:

.. list-table::
   :header-rows: 1

   * - Document
     - UK
     - Top
     - Pop
     - Coronavirus
   * - News
     - 4
     - 2
     - 0
     - 6
   * - Culture
     - 0
     - 4
     - 7
     - 0
   * - Politics
     - 3
     - 0
     - 0
     - 3
   * - Entertainment
     - 3
     - 4
     - 8
     - 1


BoW in Practice
"""""""""""""""

Enter, stage left, `scikit-learn <https://scikit-learn.org/stable/>`_\ :

.. code-block:: python

   from sklearn.feature_extraction.text import CountVectorizer
   vectorizer = CountVectorizer()

   vectorizer.fit(texts)
   vectors = vectorizer.transform(texts)

   ## Same thing:
   ## vectors = vectorizer.fit_transform(texts)

   print(f'Vocabulary: {vectorizer.vocabulary_}')
   print(f'Full vector: {vectors.toarray()}')

TF/IDF
""""""

Builds on Count Vectorisation by normalising the document frequency measure by the overall corpus frequency. Common words receive a large penalty:

$$
W(t,d) = TF(t,d) / log(N/DF_{t})
$$

For example: if the term 'cat' appears 3 times in a document of 100 words then $TF(t,d)=3/100$. If there are 10,000 documents and cat appears in 1,000 documents then $N/DF_{t}=10000/1000$ and $log(10)=1$, so IDF=1 and TF/IDF=0.03.

TF/IDF in Practice
##################

.. code-block:: python

   from sklearn.feature_extraction.text import TfidfVectorizer
   vectorizer = TfidfVectorizer()

   vectorizer.fit(texts)
   vectors = vectorizer.transform(texts)

   ## Same thing:
   ## vectors=vectorizer.fit_transform(texts)

   print(f'Vocabulary: {vectorizer.vocabulary_}')
   print(f'Full vector: {vectors.toarray()}')

^ What do you notice about how this code differs from the CountVectorizer?

Term Co-Occurence Matrix (TCM)
""""""""""""""""""""""""""""""

Three input texts:


* the cat sat on the mat
* the cat sat on the fluffy mat
* the fluffy ginger cat sat on the mat

.. list-table::
   :header-rows: 1

   * - 
     - fluffy
     - mat
     - ginger
     - sat
     - on
     - cat
     - the
   * - fluffy
     - 
     - 1
     - 1
     - 
     - 0.5
     - 0.5
     - 2.0
   * - mat
     - 
     - 
     - 
     - 
     - 0.5
     - 
     - 1.5
   * - ginger
     - 
     - 
     - 
     - 0.5
     - 0.5
     - 1.0
     - 1.5
   * - sat
     - 
     - 
     - 
     - 
     - 3.0
     - 3.0
     - 2.5
   * - on
     - 
     - 
     - 
     - 
     - 
     - 1.5
     - 3.0
   * - cat
     - 
     - 
     - 
     - 
     - 
     - 
     - 2.0
   * - the
     - 
     - 
     - 
     - 
     - 
     - 


Text2Vec & word embedding
"""""""""""""""""""""""""

Typically some kind of 2 or 3-layer neural network that 'learns' (using as big a training data set as possible) how to embed the TCM into a lower-dimension representation. 

Conceptual similarities to PCA in terms of what we're trying to achieve, but the *process* is utterly different.

Many different approaches, but `GloVe <https://nlp.stanford.edu/projects/glove/>`_ (Stanford), `word2vec <https://code.google.com/archive/p/word2vec/>`_ (Google), `fastText <https://fasttext.cc/docs/en/english-vectors.html>`_ (Facebook), and `ELMo <https://allennlp.org/elmo>`_ (Allen) or `BERT <https://github.com/google-research/bert>`_ (Google) are probably the best-known.

Sentiment Analysis
""""""""""""""""""

Requires us to deal in great detail with bi- and tri-grams because *negation* and *sarcasm* are hard. Also tends to require training/labelled data.


.. image:: http://www.zzzhou.me/images/2020/11/25/Sentiment_Analysis.png
   :target: http://www.zzzhou.me/images/2020/11/25/Sentiment_Analysis.png
   :alt: Sentiment_Analysis


`A Sentiment Analysis Approach to Predicting Stock Returns <https://medium.com/@tomyuz/a-sentiment-analysis-approach-to-predicting-stock-returns-d5ca8b75a42>`_

Clustering
""""""""""

.. list-table::
   :header-rows: 1

   * - Cluster
     - Geography
     - Earth Science
     - History
     - Computer Science
     - Total
   * - 1
     - 126
     - 310
     - 104
     - 11,018
     - 11,558
   * - 2
     - 252
     - 10,673
     - 528
     - 126
     - 11,579
   * - 3
     - 803
     - 485
     - 6,730
     - 135
     - 8,153
   * - 4
     - 100
     - 109
     - 6,389
     - 28
     - 6,626
   * - Total
     - 1,281
     - 11,577
     - 13,751
     - 11,307
     - 37,916


Topic Modelling
"""""""""""""""

Learning associations of words (or images or many other things) to hidden 'topics' that generate them:

LDA

Resources
"""""""""


* `One-Hot vs Dummy Encoding <https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn>`_
* `Categorical encoding using Label-Encoding and One-Hot-Encoder <https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd>`_
* `Count Vectorization with scikit-learn <https://towardsdatascience.com/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e>`_
* `TFIDF.com <http://www.tfidf.com/>`_
* `The TF*IDF Algorithm Explained <https://www.onely.com/blog/what-is-tf-idf/>`_
* `How to Use TfidfTransformer and TfidfVectorizer <https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/##.X7gXhhP7Tlw>`_
* `SciKit Learn Feature Extraction <https://scikit-learn.org/stable/modules/classes.html##module-sklearn.feature_extraction>`_
* `Your Guide to LDA <https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d>`_
* `Machine Learning â€” Latent Dirichlet Allocation LDA <https://jonathan-hui.medium.com/machine-learning-latent-dirichlet-allocation-lda-1d9d148f13a4>`_
* `A Beginnerâ€™s Guide to Latent Dirichlet Allocation(LDA) <https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2>`_
* `Analyzing Documents with TF-IDF <https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf>`_

Basically any of the lessons on `The Programming Historian <https://programminghistorian.org/en/lessons/>`_.

----

More Resources
~~~~~~~~~~~~~~


* `Introduction to Word Embeddings <https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc>`_
* `The Current Best of Universal Word Embeddings and Sentence Embeddings <https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a>`_
* `Using GloVe Embeddings <http://text2vec.org/glove.html>`_
* `Working with Facebook's FastText Library <https://stackabuse.com/python-for-nlp-working-with-facebook-fasttext-library/>`_
* `Word2Vec and FastText Word Embedding with Gensim <https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c>`_
* `Sentence Embeddings. Fast, please! <https://towardsdatascience.com/fse-2b1ffa791cf9>`_
* `PlasticityAI Embedding Models <https://github.com/plasticityai/magnitude>`_
* `Clustering text documents using *k*\ -means <https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html##sphx-glr-auto-examples-text-plot-document-clustering-py>`_
* `Topic extraction with Non-negative Matrix Factorization and LDA <https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html##sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py>`_

Agenda
^^^^^^

Agenda
~~~~~~


* Different plotting options: https://stackoverflow.com/a/37970713

  * Short Answer: you don't, I'm a creature of habit and lazy.
  * Longer Answer: I prefer to show you *one* way that works generally, than two or three different ways that require you to change your code for each.

* Floating Point Arithmetic

  * Hat tip for this: https://github.com/jreades/i2p/issues/15
  * Floating Point errors are a *fundamental* cause of problems in many applications and they are *hard* to debug.

Techniques
~~~~~~~~~~


* CSLs and LaTeX:

  * Short answer: `no <https://tex.stackexchange.com/a/69284>`_
  * For full LaTeX bibliographies you will need either BibLaTeX or Biber as these are more powerful and offer more options.
  * CSLs are for pandoc alone AFAIK.
  * Here's a nice RMarkdown tutorial: https://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html

* RegExes

  * Part 1: ``[A-F]`` has the special meaning 'A-through-F' (i.e. ``A|B|C|D|E|F``\ ); whereas the ``{m,n}`` syntax is for something *repeating* between ``m`` and ``n`` times (inclusive). So ``m`` and ``n`` must be numeric. I think you're being misled by Python's dictionary sytnax?
  * Part 2: ``^`` has *two* meanings:

    * Inside a ``[...]`` and as the first character it means *not* (i.e. ``[^A-F]``\ ) would negate 'A-through-F'.
    * At the start of a regular expression it means *at the start of a line* (i.e. ``r'^A'`` would match lines starting with an ``A``\ ).

* Fit/Transform

  * Fitting and transformation do not have to happen at the same time.
  * But fitting cannot be updated *afterwards* as the mapping/weights have been calculated.
  * So values that weren't fitted are likely to be dropped during the transform stage because there's no mapping/weight for them.
  * You might have separate fit and transform calls if you don't want to/need to transform all of your data at once and only want to transform parts of the full data set. So you fit once and then transform multiple times.
  * You might also have separate fit and transform calls if your data is streaming (but see limitation above).

* How you do you add a bib file to Markdown?

  * Yes, only pandoc can make sense of the bib file *in Markdown*. 
  * LaTeX can process bib files too though.

Concepts
~~~~~~~~


* Can we measure the complexity of a text?

  * Up to a point: there are all sorts of ways to measure this! What is meaningful depends on the language, sentence structure, grammar, etc.! 

* Shared readings!!!

  * https://collections.plos.org/collection/science-of-stories/
  * https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0225385
  * There are a lot more on this! Great finds though!

Triumphs
~~~~~~~~


* 
  Using ``numpy``


  * Very nice! ğŸ©tip
  * Using numpy directly will definitely represent a speed-up, but you would only benefit for queries that numpy can cope with (primarily NaNs and Numbers--which is a pretty big span!)

* 
  Pronunciation


  * Hah, hah, you're not the only one!

Workshop
^^^^^^^^
